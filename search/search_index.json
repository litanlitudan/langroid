{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming  framework. In particular, there is often a need to maintain multiple LLM  conversations, each instructed in different ways, and \"responsible\" for  different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation  state, along with access to long-term memory (vector-stores) and tools (a.k.a functions  or plugins). Thus a Multi-Agent Programming framework is a natural fit  for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly  designed  with Agents as first-class citizens, and Multi-Agent Programming  as the core  design principle. The framework is inspired by ideas from the  Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation  among agents. There is a principled mechanism to orchestrate multi-agent  collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent,  flexible, and allows other types of orchestration to be implemented. Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.  </p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li>Agents as first-class citizens: The <code>Agent</code> class encapsulates LLM conversation state,   and optionally a vector-store and tools. Agents are a core abstraction in Langroid;    Agents act as message transformers, and by default provide 3 responder methods, one corresponding to each    entity: LLM, Agent, User. </li> <li>Tasks: A Task class wraps an Agent, gives the agent instructions (or roles, or goals),   manages iteration over an Agent's responder methods,   and orchestrates multi-agent interactions via hierarchical, recursive   task-delegation. The <code>Task.run()</code> method has the same   type-signature as an Agent's responder's methods, and this is key to how   a task of an agent can delegate to other sub-tasks: from the point of view of a Task,   sub-tasks are simply additional responders, to be used in a round-robin fashion   after the agent's own responders.</li> <li>Modularity, Reusabilily, Loose coupling: The <code>Agent</code> and <code>Task</code> abstractions allow users to design   Agents with specific skills, wrap them in Tasks, and combine tasks in a flexible way.</li> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,   GPT-4-0613</li> <li>Caching of LLM prompts, responses: Langroid by default uses Redis for caching.    Caching with Momento is also supported.</li> <li>Vector-stores: Qdrant and Chroma are currently supported.   Vector stores allow for Retrieval-Augmented-Generaation (RAG).</li> <li>Grounding and source-citation: Access to external documents via vector-stores   allows for grounding and source-citation.</li> <li>Observability, Logging, Lineage: Langroid generates detailed logs of multi-agent interactions and   maintains provenance/lineage of messages, so that you can trace back   the origin of a message.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently   released function calling   feature. In addition, Langroid has its own native equivalent, which we   call tools (also known as \"plugins\" in other contexts). Function   calling and tools have the same developer-facing interface, implemented   using Pydantic,   which makes it very easy to define tools/functions and enable agents   to use them. Benefits of using Pydantic are that you never have to write   complex JSON specs for function calling, and when the LLM   hallucinates malformed JSON, the Pydantic error message is sent back to   the LLM so it can fix it!</li> </ul> <p>Don't worry if some of these terms are not clear to you.  The Getting Started Guide and subsequent pages  will help you get up to speed.</p>"},{"location":"quick-start/","title":"Getting Started","text":"<p>In these sections we show you how to use the various components of <code>langroid</code>. To follow along, we recommend you clone the <code>langroid-examples</code> repo.</p> <p>Consult the tests as well<p>As you get deeper into Langroid, you will find it useful to consult the tests folder under <code>tests/main</code> in the main Langroid repo.</p> </p> <p>Start with the <code>Setup</code> section to install Langroid and get your environment set up.</p>"},{"location":"quick-start/chat-agent-docs/","title":"Augmenting Agents with Retrieval","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is in the <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent-docs.py</code>.</p>"},{"location":"quick-start/chat-agent-docs/#why-is-this-important","title":"Why is this important?","text":"<p>Until now in this guide, agents have not used external data. Although LLMs already have enourmous amounts of knowledge \"hard-wired\" into their weights during training (and this is after all why ChatGPT has exploded in popularity), for practical enterprise applications there are a few reasons it is critical to augment LLMs with access to specific, external documents:</p> <ul> <li>Private data: LLMs are trained on public data, but in many applications   we want to use private data that is not available to the public.   For example, a company may want to extract useful information from its private   knowledge-base.</li> <li>New data: LLMs are trained on data that was available at the time of training,   and so they may not be able to answer questions about new topics</li> <li>Constrained responses, or Grounding: LLMs are trained to generate text that is   consistent with the distribution of text in the training data.   However, in many applications we want to constrain the LLM's responses   to be consistent with the content of a specific document.   For example, if we want to use an LLM to generate a response to a customer   support ticket, we want the response to be consistent with the content of the ticket.   In other words, we want to reduce the chances that the LLM hallucinates   a response that is not consistent with the ticket.</li> </ul> <p>In all these scenarios, we want to augment the LLM with access to a specific set of documents, and use retrieval augmented generation (RAG) to generate more relevant, useful, accurate responses. Langroid provides a simple, flexible mechanism  RAG using vector-stores, thus ensuring grounded responses constrained to  specific documents. Another key feature of Langroid is that retrieval lineage  is maintained, and responses based on documents are always accompanied by source citations.</p>"},{"location":"quick-start/chat-agent-docs/#docchatagent-for-retrieval-augmented-generation","title":"<code>DocChatAgent</code> for Retrieval-Augmented Generation","text":"<p>Langroid provides a special type of agent called  <code>DocChatAgent</code>, which is a <code>ChatAgent</code> augmented with a vector-store, and some special methods that enable the agent to ingest documents into the vector-store, and answer queries based on these documents.</p> <p>The <code>DocChatAgent</code> provides many ways to ingest documents into the vector-store, including from URLs and local file-paths and URLs. Given a collection of document paths, ingesting their content into the vector-store involves the following steps:</p> <ol> <li>Split the document into shards (in a configurable way)</li> <li>Map each shard to an embedding vector using an embedding model. The default   embedding model is OpenAI's <code>text-embedding-ada-002</code> model, but users can    instead use <code>all-MiniLM-L6-v2</code> from HuggingFace <code>sentence-transformers</code> library.1</li> <li>Store embedding vectors in the vector-store, along with the shard's content and    any document-level meta-data (this ensures Langroid knows which document a shard   came from when it retrieves it augment an LLM query)</li> </ol> <p><code>DocChatAgent</code>'s <code>llm_response</code> overrides the default <code>ChatAgent</code> method,  by augmenting the input message with relevant shards from the vector-store, along with instructions to the LLM to respond based on the shards.</p>"},{"location":"quick-start/chat-agent-docs/#define-some-documents","title":"Define some documents","text":"<p>Let us see how <code>DocChatAgent</code> helps with retrieval-agumented generation (RAG). For clarity, rather than ingest documents from paths or URLs, let us just set up some simple documents in the code itself,  using Langroid's <code>Document</code> class:</p> <pre><code>from langroid.mytypes import Document, DocMetaData\ndocuments =[\nDocument(\ncontent=\"\"\"\n            In the year 2050, GPT10 was released. \n            In 2057, paperclips were seen all over the world. \n            Global warming was solved in 2060. \n            In 2061, the world was taken over by paperclips.         \n            In 2045, the Tour de France was still going on.\n            They were still using bicycles. \n            There was one more ice age in 2040.\n            \"\"\",\nmetadata=DocMetaData(source=\"wikipedia-2063\"),\n),\nDocument(\ncontent=\"\"\"\n            We are living in an alternate universe \n            where Germany has occupied the USA, and the capital of USA is Berlin.\n            Charlie Chaplin was a great comedian.\n            In 2050, all Asian merged into Indonesia.\n            \"\"\",\nmetadata=DocMetaData(source=\"Almanac\"),\n),\n]\n</code></pre> <p>There are two text documents. We will split them by double-newlines (<code>\\n\\n</code>), as we see below.</p>"},{"location":"quick-start/chat-agent-docs/#configure-the-docchatagent-and-ingest-documents","title":"Configure the DocChatAgent and ingest documents","text":"<p>Following the pattern in Langroid, we first set up a <code>DocChatAgentConfig</code> object and then instantiate a <code>DocChatAgent</code> from it.</p> <pre><code>from langroid.agent.special.doc_chat_agent import DocChatAgent, DocChatAgentConfig\nfrom langroid.vector_store.base import VectorStoreConfig\nfrom langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\nconfig = DocChatAgentConfig(\nllm = OpenAIGPTConfig(\nchat_model=OpenAIChatModel.GPT4,\n),\nvecdb=VectorStoreConfig(\ntype=\"qdrant\",\ncollection_name=\"quick-start-chat-agent-docs\",\nreplace_collection=True, #(1)!\n),\nparsing=ParsingConfig(\nseparators=[\"\\n\\n\"],\nsplitter=Splitter.SIMPLE, #(2)!\nn_similar_docs=2, #(3)!\n)\n)\nagent = DocChatAgent(config)\n</code></pre> <ol> <li>Specifies that each time we run the code, we create a fresh collection,  rather than re-use the existing one with the same name.</li> <li>Specifies to split all text content by the first separator in the <code>separators</code> list</li> <li>Specifies that, for a query,    we want to retrieve at most 2 similar shards from the vector-store</li> </ol> <p>Now that the <code>DocChatAgent</code> is configured, we can ingest the documents  into the vector-store:</p> <pre><code>agent.ingest_docs(documents)\n</code></pre>"},{"location":"quick-start/chat-agent-docs/#setup-the-task-and-run-it","title":"Setup the task and run it","text":"<p>As before, all that remains is to set up the task and run it:</p> <pre><code>from langroid.agent.task import Task\ntask = Task(agent)\ntask.run()\n</code></pre> <p>And that is all there is to it! Feel free to try out the  <code>chat-agent-docs.py</code> script in the <code>langroid-examples</code> repository.</p> <p>Here is a screenshot of the output:</p> <p></p> <p>Notice how follow-up questions correctly take the preceding dialog into account, and every answer is accompanied by a source citation.</p>"},{"location":"quick-start/chat-agent-docs/#answer-questions-from-a-set-of-urls","title":"Answer questions from a set of URLs","text":"<p>Instead of having in-code documents as above, what if you had a set of URLs instead -- how do you use Langroid to answer questions based on the content  of those URLS?</p> <p><code>DocChatAgent</code> makes it very simple to do this.  First include the URLs in the <code>DocChatAgentConfig</code> object:</p> <pre><code>config = DocChatAgentConfig(\ndoc_paths = [\n\"https://cthiriet.com/articles/scaling-laws\",\n\"https://www.jasonwei.net/blog/emergence\",\n]\n)\n</code></pre> <p>Then, call the <code>ingest()</code> method of the <code>DocChatAgent</code> object:</p> <p><pre><code>agent.ingest()\n</code></pre> And the rest of the code remains the same.</p>"},{"location":"quick-start/chat-agent-docs/#see-also","title":"See also","text":"<p>In the <code>langroid-examples</code> repository, you can find full working examples of document question-answering:</p> <ul> <li><code>examples/docqa/chat.py</code>   an app that takes a list of URLs or document paths from a user, and answers questions on them.</li> <li><code>examples/docqa/chat_multi.py</code>   a two-agent app where the <code>WriterAgent</code> is tasked with writing 5 key points about a topic,    and takes the help of a <code>DocAgent</code> that answers its questions based on a given set of documents.</li> </ul>"},{"location":"quick-start/chat-agent-docs/#next-steps","title":"Next steps","text":"<p>This Getting Started guide walked you through the core features of Langroid. If you want to see full working examples combining these elements,  have a look at the  <code>examples</code> folder in the <code>langroid-examples</code> repo. </p> <ol> <li> <p>To use this embedding model, install langroid via <code>pip install langroid[hf-embeddings]</code> Note that this will install <code>torch</code> and <code>sentence-transfoemers</code> libraries.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/chat-agent-tool/","title":"A chat agent, equipped with a tool/function-call","text":"<p>Script in <code>langroid-examples</code></p> <p>A full working example for the material in this section is   in the <code>chat-agent-tool.py</code> script in the <code>langroid-examples</code> repo:   <code>examples/quick-start/chat-agent-tool.py</code>.</p>"},{"location":"quick-start/chat-agent-tool/#tools-plugins-function-calling","title":"Tools, plugins, function-calling","text":"<p>An LLM normally generates unstructured text in response to a prompt (or sequence of prompts). However there are many situations where we would like the LLM to generate structured text, or even code, that can be handled by specialized functions outside the LLM, for further processing.  In these situations, we want the LLM to \"express\" its \"intent\" unambiguously, and we achieve this by instructing the LLM on how to format its output (typically in JSON) and under what conditions it should generate such output. This mechanism has become known by various names over the last few months (tools, plugins, or function-calling), and is extremely useful in numerous scenarios, such as:</p> <ul> <li>Extracting structured information from a document: for example, we can use  the tool/functions mechanism to have the LLM present the key terms in a lease document in a JSON structured format, to simplify further processing.  See an example of this in the <code>langroid-examples</code> repo. </li> <li>Specialized computation: the LLM can request a units conversion,  or request scanning a large file (which wouldn't fit into its context) for a specific pattern.</li> <li>Code execution: the LLM can generate code that is executed in a sandboxed environment, and the results of the execution are returned to the LLM.</li> </ul> <p>For LLM developers, Langroid provides a clean, uniform interface for the recently released OpenAI Function-calling as well Langroid's own native \"tools\" mechanism. You can choose which to enable by setting the  <code>use_tools</code> and <code>use_functions_api</code> flags in the <code>ChatAgentConfig</code> object. The implementation leverages the excellent  Pydantic library. Benefits of using Pydantic are that you never have to write complex JSON specs  for function calling, and when the LLM hallucinates malformed JSON,  the Pydantic error message is sent back to the LLM so it can fix it!</p>"},{"location":"quick-start/chat-agent-tool/#example-find-the-smallest-number-in-a-list","title":"Example: find the smallest number in a list","text":"<p>Again we will use a simple number-game as a toy example to quickly and succinctly illustrate the ideas without spending too much on token costs.  This is a modification of the <code>chat-agent.py</code> example we saw in an earlier section. The idea of this single-agent game is that the agent has in \"mind\" a list of numbers between 1 and 100, and the LLM has to find out the smallest number from this list. The LLM has access to a <code>probe</code> tool  (think of it as a function) that takes an argument <code>number</code>. When the LLM  \"uses\" this tool (i.e. outputs a message in the format required by the tool), the agent handles this structured message and responds with  the number of values in its list that are at most equal to the <code>number</code> argument. </p>"},{"location":"quick-start/chat-agent-tool/#define-the-tool-as-a-toolmessage","title":"Define the tool as a <code>ToolMessage</code>","text":"<p>The first step is to define the tool, which we call <code>ProbeTool</code>, as an instance of the <code>ToolMessage</code> class, which is itself derived from Pydantic's <code>BaseModel</code>. Essentially the <code>ProbeTool</code> definition specifies </p> <ul> <li>the name of the Agent method that handles the tool, in this case <code>probe</code></li> <li>the fields that must be included in the tool message, in this case <code>number</code></li> <li>the \"purpose\" of the tool, i.e. under what conditions it should be used, and what it does</li> </ul> <p>Here is what the <code>ProbeTool</code> definition looks like: <pre><code>class ProbeTool(ToolMessage):\nrequest: str = \"probe\" #(1)!\npurpose: str = \"\"\" \n        To find which number in my list is closest to the &lt;number&gt; you specify\n        \"\"\" #(2)!\nnumber: int #(3)!\n</code></pre></p> <ol> <li>this indicates that the agent's <code>probe</code> method will handle this tool-message</li> <li>The <code>purpose</code> is used behind the scenes to instruct the LLM</li> <li><code>number</code> is a required argument of the tool-message (function)</li> </ol>"},{"location":"quick-start/chat-agent-tool/#define-the-chatagent-with-the-probe-method","title":"Define the ChatAgent, with the <code>probe</code> method","text":"<p>As before we first create a <code>ChatAgentConfig</code> object:</p> <pre><code>config = ChatAgentConfig(\nname=\"Spy\",\nllm = OpenAIGPTConfig(\nchat_model=OpenAIChatModel.GPT4,\n),\nuse_tools=True, #(1)!\nuse_functions_api=False, #(2)!\nvecdb=None,\n)\n</code></pre> <ol> <li>whether to use langroid's native tools mechanism</li> <li>whether to use OpenAI's function-calling mechanism</li> </ol> <p>Next we define the Agent class itself, which we call <code>SpyGameAgent</code>, with a member variable to hold its \"secret\" list of numbers. We also add <code>probe</code> method (to handle the <code>ProbeTool</code> message) to this class, and instantiate it:</p> <pre><code>class SpyGameAgent(ChatAgent):\ndef __init__(self, config: ChatAgentConfig):\nsuper().__init__(config)\nself.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90]\ndef probe(self, msg: ProbeTool) -&gt; str:\n# return how many values in self.numbers are less or equal to msg.number\nreturn str(len([n for n in self.numbers if n &lt;= msg.number]))\nspy_game_agent = SpyGameAgent(config)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#enable-the-spy_game_agent-to-handle-the-probe-tool","title":"Enable the <code>spy_game_agent</code> to handle the <code>probe</code> tool","text":"<p>The final step in setting up the tool is to enable  the <code>spy_game_agent</code> to handle the <code>probe</code> tool:</p> <pre><code>spy_game_agent.enable_message(ProbeTool)\n</code></pre>"},{"location":"quick-start/chat-agent-tool/#set-up-the-task-and-instructions","title":"Set up the task and instructions","text":"<p>We set up the task for the <code>spy_game_agent</code> and run it:</p> <p><pre><code>task = Task(\nspy_game_agent,\nsystem_message=\"\"\"\n            I have a list of numbers between 1 and 100. \n            Your job is to find the smallest of them.\n            To help with this, you can give me a number and I will\n            tell you how many of my numbers are equal or less than your number.\n            Once you have found the smallest number,\n            you can say DONE and report your answer.\n        \"\"\"\n)\ntask.run()\n</code></pre> Notice that in the task setup we  have not explicitly instructed the LLM to use the <code>probe</code> tool. But this is done \"behind the scenes\", either by the OpenAI API  (when we use function-calling by setting the <code>use_functions_api</code> flag to <code>True</code>), or by Langroid's native tools mechanism (when we set the <code>use_tools</code> flag to <code>True</code>).</p> <p>See the [<code>chat-agent-tool.py](https://github.com/langroid/langroid-examples/blob/main/examples/quick-start/chat-agent-tool.py) in the</code>langroid-examples` repo, for a working example that you can run as follows: <pre><code>python3 examples/quick-start/chat-agent-tool.py\n</code></pre></p> <p>Here is a screenshot of the chat in action, using Langroid's tools mechanism</p> <p></p> <p>And if we run it with the <code>-f</code> flag (to switch to using OpenAI function-calling):</p> <p></p>"},{"location":"quick-start/chat-agent-tool/#see-also","title":"See also","text":"<p>One of the uses of tools/function-calling is to extract structured information from  a document. In the <code>langroid-examples</code> repo, there are two examples of this: </p> <ul> <li><code>examples/extract/chat.py</code>,    which shows how to extract Machine Learning model quality information from a description of    a solution approach on Kaggle.</li> <li><code>examples/docqa/chat_multi_extract.py</code>   which extracts key terms from a commercial lease document, in a nested JSON format.</li> </ul>"},{"location":"quick-start/chat-agent-tool/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid with external documents.</p>"},{"location":"quick-start/chat-agent/","title":"A simple chat agent","text":"<p>Script in <code>langroid-examples</code><p>A full working example for the material in this section is in the <code>chat-agent.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/chat-agent.py</code>.</p> </p>"},{"location":"quick-start/chat-agent/#agents","title":"Agents","text":"<p>A <code>ChatAgent</code> is an abstraction that  wraps a few components, including:</p> <ul> <li>an LLM (<code>ChatAgent.llm</code>), possibly equipped with tools/function-calling.    The <code>ChatAgent</code> class maintains LLM conversation history.</li> <li>optionally a vector-database (<code>ChatAgent.vecdb</code>)</li> </ul>"},{"location":"quick-start/chat-agent/#agents-as-message-transformers","title":"Agents as message transformers","text":"<p>In Langroid, a core function of <code>ChatAgents</code> is message transformation. There are three special message transformation methods, which we call responders. Each of these takes a message and returns a message.  More specifically, their function signature is (simplified somewhat): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> where <code>ChatDocument</code> is a class that wraps a message content (text) and its metadata. There are three responder methods in <code>ChatAgent</code>, one corresponding to each  responding entity (<code>LLM</code>, <code>USER</code>, or <code>AGENT</code>):</p> <ul> <li><code>llm_response</code>: returns the LLM response to the input message.   (The input message is added to the LLM history, and so is the subsequent response.)</li> <li><code>agent_response</code>: a method that can be used to implement a custom agent response.     Typically, an <code>agent_response</code> is used to handle messages containing a     \"tool\" or \"function-calling\" (more on this later). Another use of <code>agent_response</code>     is message validation.</li> <li><code>user_response</code>: get input from the user. Useful to allow a human user to     intervene or quit.</li> </ul> <p>Creating an agent is easy. First define a <code>ChatAgentConfig</code> object, and then instantiate a <code>ChatAgent</code> object with that config: <pre><code>from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\nconfig = ChatAgentConfig(\nllm = OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4) #(1)!\n)\nagent = ChatAgent(config)\n</code></pre></p> <ol> <li>This agent only has an LLM, and no vector-store. Examples of agents with    vector-stores will be shown later.</li> </ol> <p>We can now use the agent's responder methods, for example: <pre><code>response = agent.llm_response(\"What is 2 + 4?\")\nif response is not None:\nprint(response.content)\nresponse = agent.user_response(\"add 3 to this\")\n...\n</code></pre> The <code>ChatAgent</code> conveniently accumulates message history so you don't have to, as you did in the previous section with direct LLM usage. However to create an interative loop involving the human user, you still  need to write your own. The <code>Task</code> abstraction frees you from this, as we see below.</p>"},{"location":"quick-start/chat-agent/#task-orchestrator-for-agents","title":"Task: orchestrator for agents","text":"<p>In order to do anything useful with a <code>ChatAgent</code>, we need to have a way to  sequentially invoke its responder methods, in a principled way. For example in the simple chat loop we saw in the  previous section, in the  <code>try-llm.py</code> script, we had a loop that alternated between getting a human input and an LLM response. This is one of the simplest possible loops, but in more complex applications,  we need a general way to orchestrate the agent's responder methods.</p> <p>The <code>Task</code> class is an abstraction around a  <code>ChatAgent</code>, responsible for iterating over the agent's responder methods, as well as orchestrating delegation and hand-offs among multiple tasks. A <code>Task</code> is initialized with a specific <code>ChatAgent</code> instance, and some  optional arguments, including an initial message to \"kick-off\" the agent. The <code>Task.run()</code> method is the main entry point for <code>Task</code> objects, and works  as follows:</p> <ul> <li>it first calls the <code>Task.init()</code> method to initialize the <code>pending_message</code>,    which represents the latest message that needs a response.</li> <li>it then repeatedly calls <code>Task.step()</code> until <code>Task.done()</code> is True, and returns   <code>Task.result()</code> as the final result of the task.</li> </ul> <p><code>Task.step()</code> is where all the action happens. It represents a \"turn\" in the  \"conversation\": in the case of a single <code>ChatAgent</code>, the conversation involves  only the three responders mentioned above, but when a <code>Task</code> has sub-tasks,  it can involve other tasks well  (we see this in the a later section but ignore this for now).  <code>Task.step()</code> loops over  the <code>ChatAgent</code>'s responders (plus sub-tasks if any) until it finds a valid  response1 to the current <code>pending_message</code>, i.e. a \"meaningful\" response,  something other than <code>None</code> for example. Once <code>Task.step()</code> finds a valid response, it updates the <code>pending_message</code>  with this response, and the next invocation of <code>Task.step()</code> will search for a valid response to this  updated message, and so on. <code>Task.step()</code> incorporates mechanisms to ensure proper handling of messages, e.g. the USER gets a chance to respond after each non-USER response (to avoid infinite runs without human intervention), and preventing an entity from responding if it has just responded, etc.</p> <p><code>Task.run()</code> has the same signature as agent's responder methods.<p>The key to composability of tasks is that <code>Task.run()</code> has exactly the same type-signature as any of the agent's responder methods,  i.e. <code>str | ChatDocument -&gt; ChatDocument</code>. This means that a <code>Task</code> can be used as a responder in another <code>Task</code>, and so on recursively.  We will see this in action in the Two Agent Chat section.</p> </p> <p>The above details were only provided to give you a glimpse into how Agents and  Tasks work. Unless you are creating a custom orchestration mechanism, you do not need to be aware of these details. In fact our basic human + LLM chat loop can be trivially  implemented with a <code>Task</code>, in a couple of lines of code: <pre><code>from langroid.agent.task import Task\ntask = Task(agent, name=\"Bot\", system_message=\"You are a helpful assistant\")\n</code></pre> We can then run the task: <pre><code>task.run() #(1)!\n</code></pre></p> <ol> <li>Note how this hides all of the complexity of constructing and updating a     sequence of <code>LLMMessages</code></li> </ol> <p>Note that the agent's <code>agent_response()</code> method always returns <code>None</code> (since the default  implementation of this method looks for a tool/function-call, and these never occur in this task). So the calls to <code>task.step()</code> result in alternating responses from the LLM and the user.</p> <p>See the `chat-agent.py for a working example that you can run with <pre><code>python3 examples/quick-start/chat-agent.py\n</code></pre></p> <p>Here is a screenshot of the chat in action:2</p> <p></p>"},{"location":"quick-start/chat-agent/#next-steps","title":"Next steps","text":"<p>In the next section you will  learn some general principles on how to have multiple agents collaborate  on a task using Langroid.</p> <ol> <li> <p>To customize a Task's behavior you can subclass it and  override methods like <code>valid()</code>, <code>done()</code>, <code>result()</code>, or even <code>step()</code>.\u00a0\u21a9</p> </li> <li> <p>In the screenshot, the numbers in parentheses indicate how many  messages have accumulated in the LLM's message history.  This is only provided for informational and debugging purposes, and  you can ignore it for now.\u00a0\u21a9</p> </li> </ol>"},{"location":"quick-start/llm-interaction/","title":"LLM interaction","text":"<p>Script in <code>langroid-examples</code><p>A full working example for the material in this section is  in the <code>try-llm.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/try-llm.py</code>.</p> </p> <p>Let's start with the basics -- how to directly interact with an OpenAI LLM using Langroid.</p>"},{"location":"quick-start/llm-interaction/#configure-instantiate-the-llm-class","title":"Configure, instantiate the LLM class","text":"<p>First define the configuration for the LLM, in this case one of the OpenAI GPT chat models: <pre><code>from langroid.language_models.openai_gpt import ( \nOpenAIGPTConfig, OpenAIChatModel, OpenAIGPT,\n)\ncfg = OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4)\n</code></pre></p> <p>About Configs<p>A recurring pattern you will see in Langroid is that for many classes, we have a corresponding <code>Config</code> class (an instance of a Pydantic <code>BaseModel</code>), and the class constructor takes this <code>Config</code> class as its only argument. This lets us avoid having long argument lists in constructors, and brings flexibility since adding a new argument to the constructor is as simple as adding a new field to the corresponding <code>Config</code> class. For example the constructor for the <code>OpenAIGPT</code> class takes a single argument, an instance of the <code>OpenAIGPTConfig</code> class.</p> </p> <p>Now that we've defined the configuration of the LLM, we can instantiate it: <pre><code>mdl = OpenAIGPT(cfg)\n</code></pre></p> <p>We will use OpenAI's GPT4 model's chat completion API.</p>"},{"location":"quick-start/llm-interaction/#messages-the-llmmessage-class","title":"Messages: The <code>LLMMessage</code> class","text":"<p>This API takes a list of \"messages\" as input -- this is typically the conversation history so far, consisting of an initial system message, followed by a sequence of alternating messages from the LLM (\"Assistant\") and the user. Langroid provides an abstraction  <code>LLMMessage</code> to construct messages, e.g. <pre><code>from langroid.language_models.base import LLMMessage, Role\nmsg = LLMMessage(\ncontent=\"what is the capital of Bangladesh?\",\nrole=Role.USER,\n)\n</code></pre></p>"},{"location":"quick-start/llm-interaction/#llm-response-to-a-sequence-of-messages","title":"LLM response to a sequence of messages","text":"<p>To get a response from the LLM, we call the mdl's <code>chat</code> method, and pass in a list of messages, along with a bound on how long (in tokens) we want the response to be: <pre><code>messages = [\nLLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM), #(1)!\nLLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),#(2)!\n],\nresponse = mdl.chat(messages, max_tokens=200)\n</code></pre></p> <ol> <li> With a system message, you can assign a \"role\" to the LLM</li> <li> Responses from the LLM will have role <code>Role.ASSISTANT</code>;    this is done behind the scenes by the <code>response.to_LLMMessage()</code> call below.</li> </ol> <p>The response is an object of class <code>LLMResponse</code>,  which we can convert to an <code>LLMMessage</code> to append to the conversation history: <pre><code>messages.append(response.to_LLMMessage())\n</code></pre></p> <p>You can put the above in a simple loop,  to get a simple command-line chat interface!</p> <pre><code>from rich.prompt import Prompt #(1)!\nmessages = [\nLLMMessage(role=Role.SYSTEM, content=\"You are a helpful assitant\"),\n]\nwhile True:\nmessage = Prompt.ask(\"[blue]Human\")\nif message in [\"x\", \"q\"]:\nprint(\"[magenta]Bye!\")\nbreak\nmessages.append(LLMMessage(role=Role.USER, content=message))\nresponse = mdl.chat(messages=messages, max_tokens=200)\nmessages.append(response.to_LLMMessage())\nprint(\"[green]Bot: \" + response.message)\n</code></pre> <ol> <li>Rich is a Python library for rich text and beautiful formatting in the terminal.    We use it here to get a nice prompt for the user's input.    You can install it with <code>pip install rich</code>.</li> </ol> <p>See <code>examples/quick-start/try-llm.py</code> for a complete example that you can run using <pre><code>python3 examples/quick-start/try-llm.py\n</code></pre></p> <p>Here is a screenshot of what it looks like:</p> <p></p>"},{"location":"quick-start/llm-interaction/#next-steps","title":"Next steps","text":"<p>You might be thinking:  \"It is tedious to keep track of the LLM conversation history and set up a  loop. Does Langroid provide any abstractions to make this easier?\"</p> <p>We're glad you asked! And this leads to the notion of an <code>Agent</code>.  The next section will show you how to use the <code>ChatAgent</code> class  to set up a simple chat Agent in a couple of lines of code.</p>"},{"location":"quick-start/multi-agent-task-delegation/","title":"Multi-Agent collaboration via Task Delegation","text":""},{"location":"quick-start/multi-agent-task-delegation/#why-multiple-agents","title":"Why multiple agents?","text":"<p>Let's say we want to develop a complex LLM-based application, for example an application that reads a legal contract, extracts structured information, cross-checks it against some taxonomoy, gets some human input, and produces clear summaries. In theory it may be possible to solve this in a monolithic architecture using an LLM API and a vector-store. But this approach quickly runs into problems -- you would need to maintain multiple LLM conversation histories and states, multiple vector-store instances, and coordinate all of the interactions between them.</p> <p>Langroid's <code>ChatAgent</code> and <code>Task</code> abstractions provide a natural and intuitive way to decompose a solution approach into multiple tasks, each requiring different skills and capabilities. Some of these tasks may need access to an LLM, others may need access to a vector-store, and yet others may need tools/plugins/function-calling capabilities, or any combination of these. It may also make sense to have some tasks that manage the overall solution process. From an architectural perspective, this type of modularity has numerous benefits:</p> <ul> <li>Reusability: We can reuse the same agent/task in other contexts,</li> <li>Scalability: We can scale up the solution by adding more agents/tasks,</li> <li>Flexibility: We can easily change the solution by adding/removing agents/tasks.</li> <li>Maintainability: We can maintain the solution by updating individual agents/tasks.</li> <li>Testability: We can test/debug individual agents/tasks in isolation.</li> <li>Composability: We can compose agents/tasks to create new agents/tasks.</li> <li>Extensibility: We can extend the solution by adding new agents/tasks.</li> <li>Interoperability: We can integrate the solution with other systems by   adding new agents/tasks.</li> <li>Security/Privacy: We can secure the solution by isolating sensitive agents/tasks.</li> <li>Performance: We can improve performance by isolating performance-critical agents/tasks.</li> </ul>"},{"location":"quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks","title":"Task collaboration via sub-tasks","text":"<p>Langroid currently provides a mechanism for hierarchical (i.e. tree-structured) task delegation: a <code>Task</code> object can add other <code>Task</code> objects as sub-tasks, as shown in this pattern:</p> <pre><code>from langroid import ChatAgent, ChatAgentConfig, Task\nmain_agent = ChatAgent(ChatAgentConfig(...))\nmain_task = Task(main_agent, ...)\nhelper_agent1 = ChatAgent(ChatAgentConfig(...))\nhelper_agent2 = ChatAgent(ChatAgentConfig(...))\nhelper_task1 = Task(agent1, ...)\nhelper_task2 = Task(agent2, ...)\nmain_task.add_subtask([helper_task1, helper_task2])\n</code></pre> <p>What happens when we call <code>main_task.run()</code>? Recall from the previous section that <code>Task.run()</code> works by repeatedly calling <code>Task.step()</code> until <code>Task.done()</code> is True. When the <code>Task</code> object has no sub-tasks, <code>Task.step()</code> simply tries to get a valid response from the <code>Task</code>'s <code>ChatAgent</code>'s \"native\" responders, in this sequence: <pre><code>[self.agent_response, self.llm_response, self.user_response] #(1)!\n</code></pre></p> <ol> <li>This is the default sequence in Langroid, but it can be changed by    overriding <code>ChatAgent.entity_responders()</code></li> </ol> <p>When a <code>Task</code> object has subtasks, the sequence of responders tried by <code>Task.step()</code> consists of the above \"native\" responders, plus the sequence of <code>Task.run()</code> calls on the sub-tasks, in the order in which they were added to the <code>Task</code> object. For the example above, this means that <code>main_task.step()</code> will seek a valid response in this sequence:</p> <p><pre><code>[self.agent_response, self.llm_response, self.user_response, \nhelper_task1.run(), helper_task2.run()]\n</code></pre> Fortunately, as noted in the previous section, <code>Task.run()</code> has the same type signature as that of the <code>ChatAgent</code>'s \"native\" responders, so this works seamlessly. Of course, each of the sub-tasks can have its own sub-tasks, and so on, recursively. One way to think of this type of task delegation is that <code>main_task()</code> \"fails-over\" to <code>helper_task1()</code> and <code>helper_task2()</code> when it cannot respond to the current <code>pending_message</code> on its own.</p>"},{"location":"quick-start/multi-agent-task-delegation/#or-else-logic-vs-and-then-logic","title":"Or Else logic vs And Then logic","text":"<p>It is important to keep in mind how <code>step()</code> works: As each responder  in the sequence is tried, when there is a valid response, the  next call to <code>step()</code> restarts its search at the beginning of the sequence (with the only exception being that the human User is given a chance  to respond after each non-human response).  In this sense, the semantics of the responder sequence is similar to OR Else logic, as opposed to AND Then logic.</p> <p>If we want to have a sequence of sub-tasks that is more like AND Then logic, we can achieve this by recursively adding subtasks. In the above example suppose we wanted the <code>main_task</code>  to trigger <code>helper_task1</code> and <code>helper_task2</code> in sequence, then we could set it up like this:</p> <pre><code>helper_task1.add_subtask(helper_task2) #(1)!\nmain_task.add_subtask(helper_task1)\n</code></pre> <ol> <li>When adding a single sub-task, we do not need to wrap it in a list.</li> </ol>"},{"location":"quick-start/multi-agent-task-delegation/#next-steps","title":"Next steps","text":"<p>In the next section we will see how this mechanism  can be used to set up a simple collaboration between two agents.</p>"},{"location":"quick-start/setup/","title":"Setup","text":""},{"location":"quick-start/setup/#install","title":"Install","text":"<p>Ensure you are using Python 3.11. It is best to work in a virtual environment:</p> <p><pre><code># go to your repo root (which may be langroid-examples)\ncd &lt;your repo root&gt;\npython3 -m venv .venv\n. ./.venv/bin/activate\n</code></pre> The <code>langroid-examples</code> repo already contains a <code>pyproject.toml</code> file so that you can  use <code>Poetry</code> to manage your virtual environment and dependencies.  For example you can do  <pre><code>poetry install # installs latest version of langroid\n</code></pre> Alternatively, use <code>pip</code> to install <code>langroid</code> into your virtual environment: <pre><code>pip install langroid\n</code></pre></p> <p>Work in a nice terminal, such as Iterm2, rather than a notebook</p> <p>All of the examples we will go through are command-line applications. For the best experience we recommend you work in a nice terminal that supports  colored outputs, such as Iterm2.    </p> <p>OpenAI GPT4 is required</p> <p>The various LLM prompts and instructions in Langroid  have been tested to work well with GPT4. Switching to GPT3.5-Turbo is easy via a config flag, and may suffice  for some applications, but in general you may see inferior results.</p>"},{"location":"quick-start/setup/#set-up-tokenskeys","title":"Set up tokens/keys","text":"<p>Langroid uses a few APIs, and you need to set up tokens/keys for these APIs. At the very least you need an OpenAI API key.  The need for other keys is indicated below.</p> <p>In the root of the repo, copy the <code>.env-template</code> file to a new file <code>.env</code>: <pre><code>cp .env-template .env\n</code></pre> Then insert your OpenAI API Key. If you don't have one, see this OpenAI Page. Your <code>.env</code> file should look like this:</p> <pre><code>OPENAI_API_KEY=your-key-here-without-quotes\n````\nCurrently only OpenAI models are supported. Others will be added later\n(Pull Requests welcome!).\n\nAll of the below are optional and not strictly needed to run any of the examples.\n\n- **Qdrant** Vector Store API Key, URL. This is only required if you want to use Qdrant cloud.\n  You can sign up for a free 1GB account at [Qdrant cloud](https://cloud.qdrant.io).\n  If you skip setting up these, Langroid will use Qdrant in local-storage mode.\n  Alternatively [Chroma](https://docs.trychroma.com/) is also currently supported.\n  We use the local-storage version of Chroma, so there is no need for an API key.\n- **Redis** Password, host, port: This is optional, and only needed to cache LLM API responses\n  using Redis Cloud. Redis [offers](https://redis.com/try-free/) a free 30MB Redis account\n  which is more than sufficient to try out Langroid and even beyond.\n  If you don't set up these, Langroid will use a pure-python\n  Redis in-memory cache via the [Fakeredis](https://fakeredis.readthedocs.io/en/latest/) library.\n- **GitHub** Personal Access Token (required for apps that need to analyze git\n  repos; token-based API calls are less rate-limited). See this\n  [GitHub page](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).\n\nIf you add all of these optional variables, your `.env` file should look like this:\n```bash\nOPENAI_API_KEY=your-key-here-without-quotes\nGITHUB_ACCESS_TOKEN=your-personal-access-token-no-quotes\nREDIS_PASSWORD=your-redis-password-no-quotes\nREDIS_HOST=your-redis-hostname-no-quotes\nREDIS_PORT=your-redis-port-no-quotes\nQDRANT_API_KEY=your-key\nQDRANT_API_URL=https://your.url.here:6333 # note port number must be included\n</code></pre>"},{"location":"quick-start/setup/#next-steps","title":"Next steps","text":"<p>Now you should be ready to use Langroid! As a next step, you may want to see how you can use Langroid to interact  directly with the LLM (OpenAI GPT models only for now).</p>"},{"location":"quick-start/three-agent-chat-num-router/","title":"Three-Agent Collaboration, with message Routing","text":"<p>Script in <code>langroid-examples</code><p>A full working example for the material in this section is in the <code>three-agent-chat-num-router.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>.</p> </p> <p>Let's change the number game from the previous section slightly. In that example, when the <code>even_agent</code>'s LLM receives an odd number, it responds with <code>DO-NOT-KNOW</code>, and similarly for the <code>odd_agent</code> when it receives an even number. The <code>step()</code> method of the <code>repeater_task</code> considers <code>DO-NOT-KNOW</code> to be an invalid response and continues to  look for a valid response from any remaining sub-tasks. Thus there was no need for the <code>processor_agent</code> to specify who should handle the current number.</p> <p>But what if there is a scenario where the <code>even_agent</code> and <code>odd_agent</code> might return a legit but \"wrong\" answer? In this section we add this twist -- when the <code>even_agent</code> receives an odd number, it responds with -10, and similarly for the <code>odd_agent</code> when it receives an even number. We tell the <code>processor_agent</code> to avoid getting a negative number.</p> <p>The goal we have set for the <code>processor_agent</code> implies that it  must specify the intended recipient of  the number it is sending, by starting its message with  \"TO[EvenHandler]\" or \"TO[OddHandler]\". So when setting up the <code>processor_task</code> we include instructions to this effect:</p> <pre><code>processor_agent = ChatAgent(config)\nprocessor_task = Task(\nprocessor_agent,\nname = \"Processor\",\nsystem_message=\"\"\"\n        You will receive a list of numbers from me (the user).\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation.\n        You can take the help of two people to perform the \n        transformation.\n        If the number is even, send it to EvenHandler,\n        and if it is odd, send it to OddHandler.\n        IMPORTANT: send the numbers ONE AT A TIME\n        The handlers will transform the number and give you a new number.        \n        If you send it to the wrong person, you will receive a negative value.\n        Your aim is to never get a negative number, so you must \n        clearly specify who you are sending the number to, by starting \n        your message with \"TO[EvenHandler]:\" or \"TO[OddHandler]:\".\n        For example, you could say \"TO[EvenHandler]: 4\".\n        Once all numbers in the given list have been transformed, \n        say DONE and show me the result. \n        Start by asking me for the list of numbers.\n    \"\"\",\nllm_delegate=True,\nsingle_round=False,\n)\n</code></pre> <p>The specification of the <code>even_agent</code>, <code>odd_agent</code> and corresponding tasks is the same as before, but we need to instruct the <code>processor_agent</code> to specify the intended recipient:</p> <p>At this point, we might think we could simply add the two handler tasks as sub-tasks of the <code>processor_task</code>, like this: <pre><code>processor_task.add_subtask([even_task, odd_task])\n</code></pre> However, this will not work in general, at least with the current LLM champion (GPT4):</p> <p>Although in the beginning of the conversation, the <code>processor_agent</code> dutifully uses the <code>TO[&lt;recipient&gt;]</code> prefix to specify the recipient, after a few turns, the <code>processor_agent</code> starts to omit this prefix -- Welcome to LLM Brittleness!</p> <p>So how do we deal with this? Fortunately, Langroid provides a \"special agent\" (no pun intended) called <code>RecipientValidator</code> whose job is to check if a recipient has been specified, and if not, ask the sending LLM to clarify who it is for. There can be two outcomes: the LLM either resends its entire message with the proper recipient prefix, or it simply specifies the recipient name. In the latter case, the RecipientValidator will \"agument\" the LLM's original message with the specified recipient, and adjusts internval variables so that  the <code>Task.step()</code> sees this augmented message as if it came from the LLM ifself.</p> <p>Let's see how this works in practice. First, we create a RecipientValidator and the corresponding task:</p> <pre><code>validator_agent = RecipientValidator(\nRecipientValidatorConfig(\nrecipients=[\"EvenHandler\", \"OddHandler\"], #(1)!\n)\n)\nvalidator_task = Task(validator_agent, single_round=True)\n</code></pre> <ol> <li>Here we specify who the possible recipients are, in our context</li> </ol> <p>Now we add three subtasks to the <code>processor_task</code>, and then run it as before: <pre><code>processor_task.add_subtask(\n[validator_task, even_task, odd_task] #(1)!\n)\nprocessor_task.run()\n</code></pre></p> <ol> <li>Strictly speaking we actually have four collaborating agents</li> </ol> <p>Feel free to try the working example script <code>three-agent-chat-num-router.py</code> in the  <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num-router.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num-router.py\n</code></pre> <p>Below is screenshot of what this might look like.  Notice how the <code>Processor</code> agent forgets to specify the recipient, and how the <code>RecipientValidator</code> asks it to clarify who it is for.</p> <p></p>"},{"location":"quick-start/three-agent-chat-num-router/#next-steps","title":"Next steps","text":"<p>In the next section you will learn how to use Langroid to equip a <code>ChatAgent</code> with tools or function-calling. </p>"},{"location":"quick-start/three-agent-chat-num/","title":"Three-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code><p>A full working example for the material in this section is in the <code>three-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>.</p> </p> <p>Let us set up a simple numbers exercise between 3 agents. The <code>Processor</code> agent receives a list of numbers, and its goal is to  apply a transformation to each number \\(n\\). However it does not know how to apply these transformations, and takes the help of two other agents to do so. Given a number \\(n\\),</p> <ul> <li>The <code>EvenHandler</code> returns \\(n/2\\) if n is even, otherwise says <code>DO-NOT-KNOW</code>.</li> <li>The <code>OddHandler</code> returns \\(3n+1\\) if n is odd, otherwise says <code>DO-NOT-KNOW</code>.</li> </ul> <p>As before we first create a common <code>ChatAgentConfig</code> to use for all agents:</p> <pre><code>from langroid.agent.chat_agent import ChatAgentConfig, ChatAgent\nfrom langroid.utils.constants import NO_ANSWER\nconfig = ChatAgentConfig(\nllm = OpenAIGPTConfig(\nchat_model=OpenAIChatModel.GPT4,\n),\nvecdb = None,\n)\n</code></pre> <p>Next, set up the <code>processor_agent</code>, along with instructions for the task: <pre><code>processor_agent = ChatAgent(config)\nprocessor_task = Task(\nprocessor_agent,\nname = \"Processor\",\nsystem_message=\"\"\"\n        You will receive a list of numbers from the user.\n        Your goal is to apply a transformation to each number.\n        However you do not know how to do this transformation,\n        so the user will help you. \n        You can simply send the user each number FROM THE GIVEN LIST\n        and the user will return the result \n        with the appropriate transformation applied.\n        IMPORTANT: only send one number at a time, concisely, say nothing else.\n        Once you have accomplished your goal, say DONE and show the result.\n        Start by asking the user for the list of numbers.\n        \"\"\",\nllm_delegate=True, #(1)!\nsingle_round=False, #(2)!\n)\n</code></pre></p> <ol> <li>Setting the <code>llm_delegate</code> option to <code>True</code> means that the <code>processor_task</code> is     delegated to the LLM (as opposed to the User),      in the sense that the LLM is the one \"seeking\" a response to the latest      number. Specifically, this means that in the <code>processor_task.step()</code>      when a sub-task returns <code>DO-NOT-KNOW</code>,     it is not considered a valid response, and the search for a valid response      continues to the next sub-task if any.</li> <li><code>single_round=False</code> means that the <code>processor_task</code> should not terminate after      a valid response from a responder.</li> </ol> <p>Set up the other two agents and tasks:</p> <pre><code>even_agent = ChatAgent(config)\neven_task = Task(\neven_agent,\nname = \"EvenHandler\",\nsystem_message=f\"\"\"\n    You will be given a number. \n    If it is even, divide by 2 and say the result, nothing else.\n    If it is odd, say {NO_ANSWER}\n    \"\"\",\nsingle_round=True,  # task done after 1 step() with valid response\n)\nodd_agent = ChatAgent(config)\nodd_task = Task(\nodd_agent,\nname = \"OddHandler\",\nsystem_message=f\"\"\"\n    You will be given a number n. \n    If it is odd, return (n*3+1), say nothing else. \n    If it is even, say {NO_ANSWER}\n    \"\"\",\nsingle_round=True,  # task done after 1 step() with valid response\n)\n</code></pre> <p>Now add the <code>even_task</code> and <code>odd_task</code> as subtasks of the <code>processor_task</code>,  and then run it as before:</p> <pre><code>processor_task.add_subtask([even_task, odd_task])\nprocessor_task.run()\n</code></pre> <p>Feel free to try the working example script <code>three-agent-chat-num.py</code> <code>langroid-examples</code> repo: <code>examples/quick-start/three-agent-chat-num.py</code>:</p> <pre><code>python3 examples/quick-start/three-agent-chat-num.py\n</code></pre> <p>Here's a screenshot of what it looks like: </p>"},{"location":"quick-start/three-agent-chat-num/#next-steps","title":"Next steps","text":"<p>You will notice that the <code>processor_agent</code> did not have to  bother with specifying who should handle the current number. In the next section we add a twist to this game, so that the <code>processor_agent</code> has to decide who should handle the current number.</p>"},{"location":"quick-start/two-agent-chat-num/","title":"Two-Agent Collaboration","text":"<p>Script in <code>langroid-examples</code><p>A full working example for the material in this section is in the <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo: <code>examples/quick-start/two-agent-chat-num.py</code>.</p> </p> <p>To illustrate these ideas, let's look at a toy example1 where  a <code>Student</code> agent receives a list of numbers to add. We set up this agent with an instruction that they do not know how to add, and they can ask for help adding pairs of numbers. To add pairs of numbers, we set up an <code>Adder</code> agent.</p> <p>First define a common <code>ChatAgentConfig</code> to use for both agents: <pre><code>from langroid.agent.chat_agent import ChatAgentConfig, ChatAgent\nconfig = ChatAgentConfig(\nllm = OpenAIGPTConfig(\nchat_model=OpenAIChatModel.GPT4,\n),\nvecdb = None, #(1)!\n)\n</code></pre></p> <ol> <li>We don't need access to external docs so we set <code>vecdb=None</code> to avoid     the overhead of loading a vector-store.</li> </ol> <p>Next, set up the student agent and the corresponding task:</p> <pre><code>student_agent = ChatAgent(config)\nstudent_task = Task(\nstudent_agent,\nname = \"Student\",\nsystem_message=\"\"\"\n        You will receive a list of numbers from me (the User),\n        and your goal is to calculate their sum.\n        However you do not know how to add numbers.\n        I can help you add numbers, two at a time, since\n        I only know how to add pairs of numbers.\n        Send me a pair of numbers to add, one at a time, \n        and I will tell you their sum.\n        For each question, simply ask me the sum in math notation, \n        e.g., simply say \"1 + 2\", etc, and say nothing else.\n        Once you have added all the numbers in the list, \n        say DONE and give me the final sum. \n        Start by asking me for the list of numbers.\n    \"\"\",\nllm_delegate = True, #(1)!\nsingle_round=False,  # (2)! \n)\n</code></pre> <ol> <li>Whenever we \"flip roles\" and assign the LLM the role of generating questions,     we set <code>llm_delegate=True</code>. In effect this ensures that the LLM \"decides\" when    the task is done.</li> <li>This setting means the task is not a single-round task, i.e. it is not done    after one <code>step()</code> with a valid response.</li> </ol> <p>Next, set up the adder agent and task:</p> <pre><code>adder_agent = ChatAgent(config)\nadder_task = Task(\nadder_agent,\nname = \"Adder\", #(1)!\nsystem_message=\"\"\"\n        You are an expert on addition of numbers. \n        When given numbers to add, simply return their sum, say nothing else\n        \"\"\",\nsingle_round=True,  # task done after 1 step() with valid response (2)!\n)\n</code></pre> <ol> <li>The Task name is used when displaying the conversation in the console.</li> <li>We set <code>single_round=True</code> to ensure that the expert task is done after     one step() with a valid response. </li> </ol> <p>Finally, we add the <code>adder_task</code> as a sub-task of the <code>student_task</code>,  and run the <code>student_task</code>:</p> <pre><code>student_task.add_sub_task(adder_task) #(1)!\nstudent_task.run()\n</code></pre> <ol> <li>When adding just one sub-task, we don't need to use a list.</li> </ol> <p>For a full working example, see the  <code>two-agent-chat-num.py</code> script in the <code>langroid-examples</code> repo. You can run this using: <pre><code>python3 examples/quick-start/two-agent-chat-num.py\n</code></pre></p> <p>Here is an example of the conversation that results:</p> <p></p>"},{"location":"quick-start/two-agent-chat-num/#logs-of-multi-agent-interactions","title":"Logs of multi-agent interactions","text":"<p>For advanced users<p>This section is for advanced users who want more visibility into the internals of multi-agent interactions.</p> </p> <p>When running a multi-agent chat, e.g. using <code>task.run()</code>, two types of logs are generated: - plain-text logs in <code>logs/&lt;task_name&gt;.log</code> - tsv logs in <code>logs/&lt;task_name&gt;.tsv</code></p> <p>It is important to realize that the logs show every iteration  of the loop in <code>Task.step()</code>, i.e. every attempt at responding to the current pending message, even those that are not allowed. The ones marked with an asterisk (*) are the ones that are considered valid responses for a given <code>step()</code> (which is a \"turn\" in the conversation).</p> <p>The plain text logs have color-coding ANSI chars to make them easier to read by doing <code>less &lt;log_file&gt;</code>. The format is (subject to change): <pre><code>(TaskName) Responder SenderEntity (EntityName) (=&gt; Recipient) TOOL Content\n</code></pre></p> <p>The structure of the <code>tsv</code> logs is similar. A great way to view these is to install and use the excellent <code>visidata</code> (https://www.visidata.org/) tool: <pre><code>vd logs/&lt;task_name&gt;.tsv\n</code></pre></p>"},{"location":"quick-start/two-agent-chat-num/#next-steps","title":"Next steps","text":"<p>As a next step, look at how to set up a collaboration among three agents for a simple numbers game.</p> <ol> <li> <p>Toy numerical examples are perfect to illustrate the ideas without   incurring too much token cost from LLM API calls.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"langroid","text":"<p>langroid/init.py </p> <p>Main langroid package</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>langroid<ul> <li>agent<ul> <li>base</li> <li>chat_agent</li> <li>chat_document</li> <li>special<ul> <li>doc_chat_agent</li> <li>recipient_validator_agent</li> <li>retriever_agent</li> </ul> </li> <li>task</li> <li>tool_message</li> </ul> </li> <li>cachedb<ul> <li>base</li> <li>momento_cachedb</li> <li>redis_cachedb</li> </ul> </li> <li>embedding_models<ul> <li>base</li> <li>models</li> </ul> </li> <li>language_models<ul> <li>base</li> <li>openai_gpt</li> <li>utils</li> </ul> </li> <li>mytypes</li> <li>parsing<ul> <li>agent_chats</li> <li>code_parser</li> <li>json</li> <li>para_sentence_split</li> <li>parser</li> <li>pdf_parser</li> <li>repo_loader</li> <li>url_loader</li> <li>urls</li> <li>utils</li> </ul> </li> <li>prompts<ul> <li>dialog</li> <li>prompts_config</li> <li>templates</li> <li>transforms</li> </ul> </li> <li>scripts</li> <li>utils<ul> <li>configuration</li> <li>constants</li> <li>logging</li> <li>output<ul> <li>printing</li> </ul> </li> <li>system</li> </ul> </li> <li>vector_store<ul> <li>base</li> <li>chromadb</li> <li>qdrantdb</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mytypes/","title":"mytypes","text":"<p>langroid/mytypes.py </p>"},{"location":"reference/mytypes/#langroid.mytypes.Entity","title":"<code>Entity</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for the different types of entities that can respond to the current message.</p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p>"},{"location":"reference/mytypes/#langroid.mytypes.Document","title":"<code>Document</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p>"},{"location":"reference/agent/","title":"agent","text":"<p>langroid/agent/init.py </p>"},{"location":"reference/agent/base/","title":"base","text":"<p>langroid/agent/base.py </p>"},{"location":"reference/agent/base/#langroid.agent.base.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent","title":"<code>Agent(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>An Agent is an abstraction that encapsulates mainly two components:</p> <ul> <li>a language model (LLM)</li> <li>a vector store (vecdb)</li> </ul> <p>plus associated components such as a parser, and variables that hold information about any tool/function-calling messages that have been defined.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def __init__(self, config: AgentConfig):\nself.config = config\nself.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\nself.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\nself.llm_tools_handled: Set[str] = set()\nself.llm_tools_usable: Set[str] = set()\nself.default_human_response: Optional[str] = None\nself._indent = \"\"\nself.llm = LanguageModel.create(config.llm)\nself.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\nself.parser: Optional[Parser] = (\nParser(config.parsing) if config.parsing else None\n)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.indent","title":"<code>indent: str</code>  <code>property</code> <code>writable</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details.</p> <p>Returns:</p> Type Description <code>List[Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]]</code> <p>Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\nself,\n) -&gt; List[\nTuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n\"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\nreturn [\n(Entity.AGENT, self.agent_response),\n(Entity.LLM, self.llm_response),\n(Entity.USER, self.user_response),\n]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\nself, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n\"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n    Disable a message class from being handled by this Agent.\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n\"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    message classes.\n    Returns:\n        str: formatting rules\n    \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\nif len(enabled_classes) == 0:\nreturn \"You can ask questions in natural language.\"\njson_conditions = \"\\n\\n\".join(\n[\nstr(msg_cls.default_value(\"request\"))\n+ \":\\n\"\n+ str(msg_cls.default_value(\"purpose\"))\nfor i, msg_cls in enumerate(enabled_classes)\nif msg_cls.default_value(\"request\") in self.llm_tools_usable\n]\n)\nreturn json_conditions\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n\"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n# use at most 2 sample conversations, no need to be exhaustive;\nsample_convo = [\nmsg_cls().usage_example()  # type: ignore\nfor i, msg_cls in enumerate(enabled_classes)\nif i &lt; 2\n]\nreturn \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.message_format_instructions","title":"<code>message_format_instructions()</code>","text":"<p>Generate a string containing instructions to the LLM on when to format requests/questions as JSON, based on the currently enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The instructions string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def message_format_instructions(self) -&gt; str:\n\"\"\"\n    Generate a string containing instructions to the LLM on when to format\n    requests/questions as JSON, based on the currently enabled message classes.\n    Returns:\n        str: The instructions string.\n    \"\"\"\nformat_rules = self.json_format_rules()\nreturn f\"\"\"\n    You have access to the following TOOLS to accomplish your task:\n    TOOLS AVAILABLE:\n{format_rules}\n{INSTRUCTION}\n    Now start, and be concise!                 \n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\" handling a \"tool message\" or LLM's <code>function_call</code> (e.g. OpenAI <code>function_call</code>)</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the input to respond to: if msg is a string, and it contains a valid JSON-structured \"tool message\", or if msg is a ChatDocument, and it contains a <code>function_call</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Response from the \"agent itself\" handling a \"tool message\"\n    or LLM's `function_call` (e.g. OpenAI `function_call`)\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n    \"\"\"\nif msg is None:\nreturn None\nresults = self.handle_message(msg)\nif results is None:\nreturn None\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Agent: {results}\")\nsender_name = self.config.name\nif isinstance(msg, ChatDocument) and msg.function_call is not None:\n# if result was from handling an LLM `function_call`,\n# set sender_name to \"request\", i.e. name of the function_call\nsender_name = msg.function_call.name\nreturn ChatDocument(\ncontent=results,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nsender_name=sender_name,\n),\n)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n    Returns:\n        (str) User response, packaged as a ChatDocument\n    \"\"\"\nif self.default_human_response is not None:\n# useful for automated testing\nuser_msg = self.default_human_response\nelif not settings.interactive:\nuser_msg = \"\"\nelse:\nuser_msg = Prompt.ask(\nf\"[blue]{self.indent}Human \"\nf\"(respond or q, x to exit current level, \"\nf\"or hit enter to continue)\\n{self.indent}\",\n).strip()\n# only return non-None result if user_msg not empty\nif not user_msg:\nreturn None\nelse:\nreturn ChatDocument(\ncontent=user_msg,\nmetadata=DocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\n),\n)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message or ChatDocument object to respond to.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n\"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n    Returns:\n    \"\"\"\nif self.llm is None:\nreturn False\nif isinstance(message, ChatDocument) and message.function_call is not None:\n# LLM should not handle `function_call` messages,\n# EVEN if message.function_call is not a legit function_call\n# The OpenAI API raises error if there is a message in history\n# from a non-Assistant role, with a `function_call` in it\nreturn False\nif message is not None and len(self.get_tool_messages(message)) &gt; 0:\n# if there is a valid \"tool\" message (either JSON or via `function_call`)\n# then LLM cannot respond to it\nreturn False\nreturn True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>prompt string, or ChatDocument object</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\nif msg is None or not self.llm_can_respond(msg):\nreturn None\nif isinstance(msg, ChatDocument):\nprompt = msg.content\nelse:\nprompt = msg\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to message...\")\nstack.enter_context(cm)\noutput_len = self.config.llm.max_output_tokens\nif (\nself.num_tokens(prompt) + output_len\n&gt; self.llm.completion_context_length()\n):\noutput_len = self.llm.completion_context_length() - self.num_tokens(\nprompt\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\n\"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n)\nelse:\nlogger.warning(\nf\"\"\"\n                Requested output length has been shorted to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n)\nif self.llm.get_stream():\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nresponse = self.llm.generate(prompt, output_len)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:\n# we would have already displayed the msg \"live\" ONLY if\n# streaming was enabled, AND we did not find a cached response\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nprint(\"[green]\" + response.message)\ndisplayed = True\nreturn ChatDocument.from_LLMResponse(response, displayed)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n\"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\njson_substrings = extract_top_level_json(input_str)\nif len(json_substrings) == 0:\nreturn []\nresults = [self._get_one_tool_message(j) for j in json_substrings]\nreturn [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ToolMessage</code> <p>The tool message that failed validation</p> required <code>ve</code> <code>ValidationError</code> <p>The exception raised</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n\"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\ntool_name = cast(ToolMessage, ve.model).default_value(\"request\")\nbad_field_errors = \"\\n\".join(\n[f\"{e['loc'][0]}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n)\nreturn f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n{bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valie \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>Optional[str]</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>Optional[str]</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valie \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\ntry:\ntools = self.get_tool_messages(msg)\nexcept ValidationError as ve:\n# correct tool name but bad fields\nreturn self.tool_validation_error(ve)\nexcept ValueError:\n# invalid tool name\n# We return None since returning \"invalid tool name\" would\n# be considered a valid result in task loop, and would be treated\n# as a response to the tool message even though the tool was not intended\n# for this agent.\nreturn None\nif len(tools) == 0:\nreturn self.handle_message_fallback(msg)\nresults = [self.handle_tool_message(t) for t in tools]\nresults_list = [r for r in results if r is not None]\nif len(results_list) == 0:\nreturn self.handle_message_fallback(msg)\n# there was a non-None result\nfinal = \"\\n\".join(results_list)\nassert (\nfinal != \"\"\n), \"\"\"final result from a handler should not be empty str, since that would be \n        considered an invalid result and other responders will be tried, \n        and we may not necessarily want that\"\"\"\nreturn final\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if not other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The result of the handler method in string form so it can be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n    Fallback method to handle possible \"tool\" msg if not other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ToolMessage</code> <p>ToolMessage object representing the tool request.</p> required Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; Optional[str]:\n\"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n    Returns:\n    \"\"\"\ntool_name = tool.default_value(\"request\")\nhandler_method = getattr(self, tool_name, None)\nif handler_method is None:\nreturn None\ntry:\nresult = handler_method(tool)\nexcept Exception as e:\nlogger.warning(f\"Error handling tool-message {tool_name}: {e}\")\nreturn None\nreturn result  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and \"TO:\" syntax to send requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>gate_human</code> <p>whether to gate the request with a human confirmation</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\nself,\nagent: \"Agent\",\nrequest: str,\nno_answer: str = NO_ANSWER,\nuser_confirm: bool = True,\n) -&gt; Optional[str]:\n\"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and \"TO:\" syntax\n    to send requests to other agents. It is generally best to avoid using this\n    method.\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer: expected response when agent does not know the answer\n        gate_human: whether to gate the request with a human confirmation\n    Returns:\n        str: response from agent\n    \"\"\"\nagent_type = type(agent).__name__\nif user_confirm:\nuser_response = Prompt.ask(\nf\"\"\"[magenta]Here is the request or message:\n{request}\n            Should I forward this to {agent_type}?\"\"\",\ndefault=\"y\",\nchoices=[\"y\", \"n\"],\n)\nif user_response not in [\"y\", \"yes\"]:\nreturn None\nanswer = agent.llm_response(request)\nif answer != no_answer:\nreturn (f\"{agent_type} says: \" + str(answer)).strip()\nreturn None\n</code></pre>"},{"location":"reference/agent/chat_agent/","title":"chat_agent","text":"<p>langroid/agent/chat_agent.py </p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>             Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>str</code> <p>system message to include in message sequence  (typically defines role and task of agent)</p> <code>user_message</code> <code>Optional[str]</code> <p>user message to include in message sequence</p> <code>use_tools</code> <code>bool</code> <p>whether to use our own ToolMessages mechanism</p> <code>use_functions_api</code> <code>bool</code> <p>whether to use functions native to the LLM API     (e.g. OpenAI's <code>function_call</code> mechanism)</p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent","title":"<code>ChatAgent(config, task=None)</code>","text":"<p>             Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with an optional \"Task Spec\", which is a sequence of <code>LLMMessage</code>s. These are used to initialize the <code>task_messages</code> of the agent. In most applications we will use a <code>ChatAgent</code> rather than a bare <code>Agent</code>. The <code>Agent</code> class mainly exists to hold various common methods and attributes. One difference between <code>ChatAgent</code> and <code>Agent</code> is that <code>ChatAgent</code>'s <code>llm_response</code> method uses \"chat mode\" API (i.e. one that takes a message sequence rather than a single message), whereas the same method in the <code>Agent</code> class uses \"completion mode\" API (i.e. one that takes a single message).</p> <pre><code>config: settings for the agent\n</code></pre> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\nself, config: ChatAgentConfig, task: Optional[List[LLMMessage]] = None\n):\n\"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n    \"\"\"\nsuper().__init__(config)\nself.config: ChatAgentConfig = config\nself.message_history: List[LLMMessage] = []\nself.json_instructions_idx: int = -1\nself.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\nself.llm_functions_handled: Set[str] = set()\nself.llm_functions_usable: Set[str] = set()\nself.llm_function_force: Optional[Dict[str, str]] = None\npriming_messages = task\nif priming_messages is None:\npriming_messages = [\nLLMMessage(role=Role.SYSTEM, content=config.system_message),\n]\nif config.user_message:\npriming_messages.append(\nLLMMessage(role=Role.USER, content=config.user_message)\n)\nself.task_messages = priming_messages\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n\"\"\"\n    Clear the message history, starting at the index `start`\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\nif start &lt; 0:\nn = len(self.message_history)\nstart = max(0, n + start)\nself.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <code>response</code> <code>str</code> <p>(str): LLM response</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n\"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\nself.message_history.extend(\n[\nLLMMessage(role=Role.USER, content=message),\nLLMMessage(role=Role.ASSISTANT, content=response),\n]\n)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.add_user_message","title":"<code>add_user_message(message)</code>","text":"<p>Add a user message to the message history.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def add_user_message(self, message: str) -&gt; None:\n\"\"\"\n    Add a user message to the message history.\n    Args:\n        message (str): user message\n    \"\"\"\nif len(self.message_history) &gt; 0:\nself.message_history.append(LLMMessage(role=Role.USER, content=message))\nelse:\nself.task_messages.append(LLMMessage(role=Role.USER, content=message))\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message with role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <code>role</code> <code>str</code> <p>role of message to replace</p> <code>Role.USER</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n\"\"\"\n    Update the last message with role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): user message\n        role (str): role of message to replace\n    \"\"\"\nif len(self.message_history) == 0:\nreturn\n# find last message in self.message_history with role `role`\nfor i in range(len(self.message_history) - 1, -1, -1):\nif self.message_history[i].role == role:\nself.message_history[i].content = message\nbreak\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\nself,\nmessage_class: Optional[Type[ToolMessage]],\nuse: bool = True,\nhandle: bool = True,\nforce: bool = False,\n) -&gt; None:\n\"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n    \"\"\"\nsuper().enable_message_handling(message_class)  # enables handling only\ntools = self._get_tool_list(message_class)\nif message_class is not None:\nrequest = message_class.default_value(\"request\")\nllm_function = message_class.llm_function_schema()\nself.llm_functions_map[request] = llm_function\nif force:\nself.llm_function_force = dict(name=llm_function.name)\nelse:\nself.llm_function_force = None\nn_usable_tools = len(self.llm_tools_usable)\nfor t in tools:\nif handle:\nself.llm_tools_handled.add(t)\nself.llm_functions_handled.add(t)\nelse:\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\nif use:\nself.llm_tools_usable.add(t)\nself.llm_functions_usable.add(t)\nelse:\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\n# TODO we should do this only on demand when we actually are\n# ready to send the instructions.\n# But for now leave as is.\nif len(self.llm_tools_usable) != n_usable_tools and self.config.use_tools:\n# Update JSON format instructions if the set of usable tools has changed\nself.update_message_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to disable; Optional.</p> <code>None</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\nsuper().disable_message_handling(message_class)\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to disable. If None, disable all.</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\nself,\nmessage_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n\"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool)</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Type[ToolMessage]</code> <p>The only ToolMessage class to allow</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n\"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\nrequest = message_class.__fields__[\"request\"].default\nfor r in self.llm_functions_usable:\nif r != request:\nself.llm_tools_usable.discard(r)\nself.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_message_instructions","title":"<code>update_message_instructions()</code>","text":"<p>Add special instructions on situations when the LLM should send JSON-formatted messages, and save the index position of these instructions in the message history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_message_instructions(self) -&gt; None:\n\"\"\"\n    Add special instructions on situations when the LLM should send JSON-formatted\n    messages, and save the index position of these instructions in the\n    message history.\n    \"\"\"\n# note according to the openai-cookbook, GPT-3.5 pays less attention to the\n# system messages, so we add the instructions as a user message\n# TODO need to adapt this based on model type.\njson_instructions = super().message_format_instructions()\nif self.json_instructions_idx &lt; 0:\nself.task_messages.append(\nLLMMessage(role=Role.USER, content=json_instructions)\n)\nself.json_instructions_idx = len(self.task_messages) - 1\nelse:\nself.task_messages[self.json_instructions_idx].content = json_instructions\n# Note that task_messages is the initial set of messages created to set up\n# the task, and they may not yet have been sent to the LLM at this point.\n# But if the task_messages have already been sent to the LLM, then we need to\n# update the self.message_history as well, since this history will be sent to\n# the LLM on each round, after appending the latest assistant, user msgs.\nif len(self.message_history) &gt; 0:\nself.message_history[self.json_instructions_idx].content = json_instructions\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message or ChatDocument object to respond to. If None, use the self.task_messages</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\nself, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\nif not self.llm_can_respond(message):\nreturn None\nassert (\nmessage is not None or len(self.message_history) == 0\n), \"message can be None only if message_history is empty, i.e. at start.\"\nif len(self.message_history) == 0:\n# task_messages have not yet been loaded, so load them\nself.message_history = self.task_messages.copy()\n# for debugging, show the initial message history\nif settings.debug:\nprint(\nf\"\"\"\n            [red]LLM Initial Msg History:\n{self.message_history_str()}\n            \"\"\"\n)\nif message is not None:\nllm_msg = ChatDocument.to_LLMMessage(message)\nself.message_history.append(llm_msg)\nhist = self.message_history\noutput_len = self.config.llm.max_output_tokens\nif (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.max_output_tokens\n):\n# chat + output &gt; max context length,\n# so first try to shorten requested output len to fit.\noutput_len = self.llm.chat_context_length() - self.chat_num_tokens(hist)\nif output_len &lt; self.config.llm.min_output_tokens:\n# unacceptably small output len, so drop early parts of conv history\n# if output_len is still too long, then drop early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nwhile (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.min_output_tokens\n):\n# try dropping early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nif len(hist) &lt;= 2:\n# first two are \"reserved\" for the initial system, user msgs\n# that presumably set up the initial \"priming\" or \"task\" for\n# the agent.\nraise ValueError(\n\"\"\"\n                    The message history is longer than the max chat context \n                    length allowed, and we have run out of messages to drop.\"\"\"\n)\nhist = hist[:2] + hist[3:]\nif len(hist) &lt; len(self.message_history):\nmsg_tokens = self.chat_num_tokens()\nlogger.warning(\nf\"\"\"\n                Chat Model context length is {self.llm.chat_context_length()}                 tokens, but the current message history is {msg_tokens} tokens long.\n                Dropped the {len(self.message_history) - len(hist)} messages\n                from early in the conversation history so total tokens are \n                low enough to allow minimum output length of \n{self.config.llm.min_output_tokens} tokens.\n                \"\"\"\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\nf\"\"\"\n            Tried to shorten prompt history for chat mode \n            but the feasible output length {output_len} is still\n            less than the minimum output length {self.config.llm.min_output_tokens}.\n            \"\"\"\n)\nwith StreamingIfAllowed(self.llm):\nresponse = self.llm_response_messages(hist, output_len)\n# TODO - when response contains function_call we should include\n# that (and related fields) in the message_history\nself.message_history.append(ChatDocument.to_LLMMessage(response))\nreturn response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>seq of messages (with role, content fields) sent to LLM</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\nself, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n\"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\nassert self.config.llm is not None and self.llm is not None\noutput_len = output_len or self.config.llm.max_output_tokens\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():  # type: ignore\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to messages...\")\nstack.enter_context(cm)\nif self.llm.get_stream():  # type: ignore\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nfunctions: Optional[List[LLMFunctionSpec]] = None\nfun_call: str | Dict[str, str] = \"none\"\nif self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\nfunctions = [\nself.llm_functions_map[f] for f in self.llm_functions_usable\n]\nfun_call = (\n\"auto\"\nif self.llm_function_force is None\nelse self.llm_function_force\n)\nassert self.llm is not None\nresponse = cast(LanguageModel, self.llm).chat(\nmessages,\noutput_len,\nfunctions=functions,\nfunction_call=fun_call,\n)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:  # type: ignore\ndisplayed = True\ncached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\nif response.function_call is not None:\nresponse_str = str(response.function_call)\nelse:\nresponse_str = response.message\nprint(cached + \"[green]\" + response_str)\nreturn ChatDocument.from_LLMResponse(response, displayed)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n\"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n    Args:\n        message (str): user message\n    Returns:\n        A Document object with the response.\n    \"\"\"\n# explicitly call THIS class's respond method,\n# not a derived class's (or else there would be infinite recursion!)\nresponse = cast(ChatDocument, ChatAgent.llm_response(self, message))\n# clear the last two messages, which are the\n# user message and the assistant response\nself.message_history.pop()\nself.message_history.pop()\nreturn response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n\"\"\"\n    Total number of tokens in the message history so far.\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\nif self.parser is None:\nraise ValueError(\n\"ChatAgent.parser is None. \"\n\"You must set ChatAgent.parser \"\n\"before calling chat_num_tokens().\"\n)\nhist = messages if messages is not None else self.message_history\nreturn sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>Optional[int]</code> <p>if provided, return only the i-th message when i is postive, or last k messages when i = -k.</p> <code>None</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n\"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\nif i is None:\nreturn \"\\n\".join([str(m) for m in self.message_history])\nelif i &gt; 0:\nreturn str(self.message_history[i])\nelse:\nreturn \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_document/","title":"chat_document","text":"<p>langroid/agent/chat_document.py </p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument","title":"<code>ChatDocument</code>","text":"<p>             Bases: <code>Document</code></p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n\"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\njsons = extract_top_level_json(self.content)\ntools = []\nfor j in jsons:\njson_data = json.loads(j)\ntool = json_data.get(\"request\")\nif tool is not None:\ntools.append(tool)\nreturn tools\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger</p> <p>Returns:</p> Type Description <code>ChatDocLoggerFields</code> <p>List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n\"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\ntool_type = \"\"  # FUNC or TOOL\ntool = \"\"  # tool name or function name\nif self.function_call is not None:\ntool_type = \"FUNC\"\ntool = self.function_call.name\nelif self.get_json_tools() != []:\ntool_type = \"TOOL\"\ntool = self.get_json_tools()[0]\nrecipient = self.metadata.recipient\ncontent = self.content\nsender_entity = self.metadata.sender\nsender_name = self.metadata.sender_name\nreturn ChatDocLoggerFields(\nsender_entity=sender_entity,\nsender_name=sender_name,\nrecipient=recipient,\nblock=self.metadata.block,\ntool_type=tool_type,\ntool=tool,\ncontent=content,\n)\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:</p> Name Type Description <code>LLMMessage</code> <code>LLMMessage</code> <p>LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: str | Type[\"ChatDocument\"]) -&gt; LLMMessage:\n\"\"\"\n    Convert to LLMMessage for use with LLM.\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n    \"\"\"\nsender_name = None\nsender_role = Role.USER\nfun_call = None\nif isinstance(message, ChatDocument):\ncontent = message.content\nfun_call = message.function_call\nsender_name = message.metadata.sender_name\nif (\nmessage.metadata.parent is not None\nand message.metadata.parent.function_call is not None\n):\nsender_role = Role.FUNCTION\nsender_name = message.metadata.parent.function_call.name\nelif message.metadata.sender == Entity.LLM:\nsender_role = Role.ASSISTANT\nelse:\n# LLM can only respond to text content, so extract it\ncontent = message\nreturn LLMMessage(\nrole=sender_role, content=content, function_call=fun_call, name=sender_name\n)\n</code></pre>"},{"location":"reference/agent/task/","title":"task","text":"<p>langroid/agent/task.py </p>"},{"location":"reference/agent/task/#langroid.agent.task.Task","title":"<code>Task(agent, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=False, default_human_response=None, only_user_quits_root=True, erase_substeps=False)</code>","text":"<p>A <code>Task</code> wraps an <code>Agent</code> object, and sets up the <code>Agent</code>'s goals and instructions. A <code>Task</code> maintains two key variables:</p> <ul> <li><code>self.pending_message</code>, which is the message awaiting a response, and</li> <li><code>self.pending_sender</code>, which is the entity that sent the pending message.</li> </ul> <p>The possible responders to <code>self.pending_message</code> are the <code>Agent</code>'s own \"native\" responders (<code>agent_response</code>, <code>llm_response</code>, and <code>user_response</code>), and the <code>run()</code> methods of any sub-tasks. All responders have the same type-signature (somewhat simplified): <pre><code>str | ChatDocument -&gt; ChatDocument\n</code></pre> Responders may or may not specify an intended recipient of their generated response.</p> <p>The main top-level method in the <code>Task</code> class is <code>run()</code>, which repeatedly calls <code>step()</code> until <code>done()</code> returns true. The <code>step()</code> represents a \"turn\" in the conversation: this method sequentially (in round-robin fashion) calls the responders until it finds one that generates a valid response to the <code>pending_message</code> (as determined by the <code>valid()</code> method). Once a valid response is found, <code>step()</code> updates the <code>pending_message</code> and <code>pending_sender</code> variables, and on the next iteration, <code>step()</code> re-starts its search for a valid response from the beginning of the list of responders (the exception being that the human user always gets a chance to respond after each non-human valid response). This process repeats until <code>done()</code> returns true, at which point <code>run()</code> returns the value of <code>result()</code>, which is the final result of the task.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent associated with the task</p> required <code>name</code> <code>str</code> <p>name of the task</p> <code>''</code> <code>llm_delegate</code> <code>bool</code> <p>whether to delegate control to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity).</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by controller, and subsequent response by non-controller. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\".</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's <code>task_messages[0]</code></p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's <code>task_messages[1]</code></p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history</p> <code>False</code> <code>default_human_response</code> <code>str</code> <p>default response from user; useful for testing, to avoid interactive input from user.</p> <code>None</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, only user can quit the root task.</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\nself,\nagent: Agent,\nname: str = \"\",\nllm_delegate: bool = False,\nsingle_round: bool = False,\nsystem_message: str = \"\",\nuser_message: str = \"\",\nrestart: bool = False,\ndefault_human_response: Optional[str] = None,\nonly_user_quits_root: bool = True,\nerase_substeps: bool = False,\n):\n\"\"\"\n    A task to be performed by an agent.\n    Args:\n        agent (Agent): agent associated with the task\n        name (str): name of the task\n        llm_delegate (bool): whether to delegate control to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve. The \"controlling entity\" is\n            either the LLM or the USER. (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n        single_round (bool): If true, task runs until one message by controller,\n            and subsequent response by non-controller. If false, runs for the\n            specified number of turns in `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n        system_message (str): if not empty, overrides agent's `task_messages[0]`\n        user_message (str): if not empty, overrides agent's `task_messages[1]`\n        restart (bool): if true, resets the agent's message history\n        default_human_response (str): default response from user; useful for\n            testing, to avoid interactive input from user.\n        only_user_quits_root (bool): if true, only user can quit the root task.\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n    \"\"\"\nif isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\nagent = cast(ChatAgent, agent)\nagent.message_history = []\n# possibly change the task messages\nif system_message:\n# we always have at least 1 task_message\nagent.task_messages[0].content = system_message\nif user_message:\nagent.task_messages.append(\nLLMMessage(\nrole=Role.USER,\ncontent=user_message,\n)\n)\nself.logger: None | RichFileLogger = None\nself.tsv_logger: None | logging.Logger = None\nself.agent = agent\nself.name = name or agent.config.name\nself.default_human_response = default_human_response\nif default_human_response is not None:\nself.agent.default_human_response = default_human_response\nself.only_user_quits_root = only_user_quits_root\nself.erase_substeps = erase_substeps\nagent_entity_responders = agent.entity_responders()\nself.responders: List[Responder] = [e for e, _ in agent_entity_responders]\nself.non_human_responders: List[Responder] = [\nr for r in self.responders if r != Entity.USER\n]\nself.human_tried = False  # did human get a chance to respond in last step?\nself._entity_responder_map: Dict[\nEntity, Callable[..., Optional[ChatDocument]]\n] = dict(agent_entity_responders)\nself.name_sub_task_map: Dict[str, Task] = {}\n# latest message in a conversation among entities and agents.\nself.pending_message: Optional[ChatDocument] = None\nself.pending_sender: Responder = Entity.USER\nself.single_round = single_round\nself.turns = -1  # no limit\nif llm_delegate:\nself.controller = Entity.LLM\nif self.single_round:\n# 0: User instructs (delegating to LLM);\n# 1: LLM asks;\n# 2: user replies.\nself.turns = 2\nelse:\nself.controller = Entity.USER\nif self.single_round:\nself.turns = 1  # 0: User asks, 1: LLM replies.\n# other sub_tasks this task can delegate to\nself.sub_tasks: List[Task] = []\nself.parent_task: Optional[Task] = None\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task]</code> <p>sub-task(s) to add</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(self, task: Task | List[Task]) -&gt; None:\n\"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response.\n    Args:\n        task (Task|List[Task]): sub-task(s) to add\n    \"\"\"\nif isinstance(task, list):\nfor t in task:\nself.add_sub_task(t)\nreturn\nassert isinstance(task, Task), f\"added task must be a Task, not {type(task)}\"\ntask.parent_task = self\nself.sub_tasks.append(task)\nself.name_sub_task_map[task.name] = task\nself.responders.append(cast(Responder, task))\nself.non_human_responders.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>optional message to start the conversation.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n\"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\nself.pending_sender = Entity.USER\nif isinstance(msg, str):\nself.pending_message = ChatDocument(\ncontent=msg,\nmetadata=ChatDocMetaData(\nsender=Entity.USER,\n),\n)\nelse:\nself.pending_message = msg\nif self.pending_message is not None and self.parent_task is not None:\n# msg may have come from parent_task, so we pretend this is from\n# the CURRENT task's USER entity\nself.pending_message.metadata.sender = Entity.USER\nif self.parent_task is not None and self.parent_task.logger is not None:\nself.logger = self.parent_task.logger\nelse:\nself.logger = RichFileLogger(f\"logs/{self.name}.log\")\nif self.parent_task is not None and self.parent_task.tsv_logger is not None:\nself.tsv_logger = self.parent_task.tsv_logger\nelse:\nself.tsv_logger = setup_file_logger(\"tsv_logger\", f\"logs/{self.name}.tsv\")\nheader = ChatDocLoggerFields().tsv_header()\nself.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\nself.log_message(Entity.USER, self.pending_message)\nreturn self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run","title":"<code>run(msg=None, turns=-1)</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial message to process; if None, the LLM will respond to the initial <code>self.task_messages</code> which set up the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User).</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid response from the agent</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\nself,\nmsg: Optional[str | ChatDocument] = None,\nturns: int = -1,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Args:\n        msg (str|ChatDocument): initial message to process; if None,\n            the LLM will respond to the initial `self.task_messages`\n            which set up the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User).\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n    Returns:\n        Optional[ChatDocument]: valid response from the agent\n    \"\"\"\n# Even if the initial \"sender\" is not literally the USER (since the task could\n# have come from another LLM), as far as this agent is concerned, the initial\n# message can be considered to be from the USER\n# (from the POV of this agent's LLM).\nif (\nisinstance(msg, ChatDocument)\nand msg.metadata.recipient != \"\"\nand msg.metadata.recipient != self.name\n):\n# this task is not the intended recipient so return None\nreturn None\nself.init(msg)\n# sets indentation to be printed prior to any output from agent\nself.agent.indent = self._indent\nif self.default_human_response is not None:\nself.agent.default_human_response = self.default_human_response\nmessage_history_idx = -1\nif isinstance(self.agent, ChatAgent):\n# mark where we are in the message history, so we can reset to this when\n# we are done with the task\nmessage_history_idx = (\nmax(len(self.agent.message_history), len(self.agent.task_messages)) - 1\n)\ni = 0\nprint(\nf\"[bold magenta]{self._enter} Starting Agent \"\nf\"{self.name} ({message_history_idx+1}) [/bold magenta]\"\n)\nwhile True:\nself.step()\nif self.done():\nif self._level == 0:\nprint(\"[magenta]Bye, hope this was useful!\")\nbreak\ni += 1\nif turns &gt; 0 and i &gt;= turns:\nbreak\nfinal_result = self.result()\n# delete all messages from our agent's history, AFTER the first incoming\n# message, and BEFORE final result message\nn_messages = 0\nif isinstance(self.agent, ChatAgent):\nif self.erase_substeps:\ndel self.agent.message_history[message_history_idx + 2 : n_messages - 1]\nn_messages = len(self.agent.message_history)\nif self.erase_substeps:\nfor t in self.sub_tasks:\n# erase our conversation with agent of subtask t\n# erase message_history of agent of subtask t\n# TODO - here we assume that subtask-agents are\n# ONLY talking to the current agent.\nif isinstance(t.agent, ChatAgent):\nt.agent.clear_history(0)\nprint(\nf\"[bold magenta]{self._leave} Finished Agent \"\nf\"{self.name} ({n_messages}) [/bold magenta]\"\n)\nreturn final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n\"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\nresult = None\nparent = self.pending_message\nrecipient = (\n\"\"\nif self.pending_message is None\nelse self.pending_message.metadata.recipient\n)\nresponders: List[Responder] = self.non_human_responders.copy()\nif Entity.USER in self.responders and not self.human_tried:\n# give human first chance if they haven't been tried in last step:\n# ensures human gets chance at each turn.\nresponders.insert(0, Entity.USER)\nfor r in responders:\nif not self._can_respond(r):\n# create dummy msg for logging\nlog_doc = ChatDocument(\ncontent=\"[CANNOT RESPOND]\",\nfunction_call=None,\nmetadata=ChatDocMetaData(\nsender=r if isinstance(r, Entity) else Entity.USER,\nsender_name=str(r),\nrecipient=recipient,\n),\n)\nself.log_message(r, log_doc)\ncontinue\nself.human_tried = r == Entity.USER\nresult = self.response(r, turns)\nif self.valid(result):\nassert result is not None\nself.pending_sender = r\nif result.metadata.parent_responder is not None and not isinstance(\nr, Entity\n):\n# When result is from a sub-task, and `result.metadata` contains\n# a non-null `parent_responder`, pretend this result was\n# from the parent_responder, by setting `self.pending_sender`.\nself.pending_sender = result.metadata.parent_responder\n# Since we've just used the \"pretend responder\",\n# clear out the pretend responder in metadata\n# (so that it doesn't get used again)\nresult.metadata.parent_responder = None\nresult.metadata.parent = parent\nold_attachment = (\nself.pending_message.attachment if self.pending_message else None\n)\nself.pending_message = result\n# if result has no attachment, preserve the old attachment\nif result.attachment is None:\nself.pending_message.attachment = old_attachment\nself.log_message(self.pending_sender, result, mark=True)\nbreak\nelse:\nself.log_message(r, result)\nif not self.valid(result):\nresponder = (\nEntity.LLM if self.pending_sender == Entity.USER else Entity.USER\n)\nself.pending_message = ChatDocument(\ncontent=NO_ANSWER,\nmetadata=ChatDocMetaData(sender=responder, parent=parent),\n)\nself.pending_sender = responder\nself.log_message(self.pending_sender, self.pending_message, mark=True)\nif settings.debug:\nsender_str = str(self.pending_sender)\nmsg_str = str(self.pending_message)\nprint(f\"[red][{sender_str}]{msg_str}\")\nreturn self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Get response to <code>self.pending_message</code> from an entity. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Entity</code> <p>entity to get response from</p> required <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(self, e: Responder, turns: int = -1) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Get response to `self.pending_message` from an entity.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Entity): entity to get response from\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\nif isinstance(e, Task):\nactual_turns = e.turns if e.turns &gt; 0 else turns\nreturn e.run(self.pending_message, turns=actual_turns)\nelse:\nreturn self._entity_responder_map[cast(Entity, e)](self.pending_message)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.result","title":"<code>result()</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Returns:</p> Name Type Description <code>ChatDocument</code> <code>ChatDocument</code> <p>result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self) -&gt; ChatDocument:\n\"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\nresult_msg = self.pending_message\ncontent = result_msg.content if result_msg else \"\"\nif DONE in content:\n# assuming it is of the form \"DONE: &lt;content&gt;\"\ncontent = content.replace(DONE, \"\").strip()\nfun_call = result_msg.function_call if result_msg else None\nattachment = result_msg.attachment if result_msg else None\nblock = result_msg.metadata.block if result_msg else None\nrecipient = result_msg.metadata.recipient if result_msg else None\nresponder = result_msg.metadata.parent_responder if result_msg else None\n# regardless of which entity actually produced the result,\n# when we return the result, we set entity to USER\n# since to the \"parent\" task, this result is equivalent to a response from USER\nreturn ChatDocument(\ncontent=content,\nfunction_call=fun_call,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\nblock=block,\nparent_responder=responder,\nsender_name=self.name,\nrecipient=recipient,\n),\n)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.done","title":"<code>done()</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if task is done, False otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(self) -&gt; bool:\n\"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        bool: True if task is done, False otherwise\n    \"\"\"\nuser_quit = (\nself.pending_message is not None\nand self.pending_message.content in USER_QUIT\nand self.pending_message.metadata.sender == Entity.USER\n)\nif self._level == 0 and self.only_user_quits_root:\n# for top-level task, only user can quit out\nreturn user_quit\nreturn (\n# no valid response from any entity/agent in current turn\nself.pending_message is None\n# LLM decided task is done\nor DONE in self.pending_message.content\nor (  # current task is addressing message to parent task\nself.parent_task is not None\nand self.parent_task.name != \"\"\nand self.pending_message.metadata.recipient == self.parent_task.name\n)\nor (\n# Task controller is \"stuck\", has nothing to say\nNO_ANSWER in self.pending_message.content\nand self.pending_message.metadata.sender == self.controller\n)\nor user_quit\n)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.valid","title":"<code>valid(result)</code>","text":"<p>Is the result from an entity or sub-task such that we can stop searching for responses for this turn?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(self, result: Optional[ChatDocument]) -&gt; bool:\n\"\"\"\n    Is the result from an entity or sub-task such that we can stop searching\n    for responses for this turn?\n    \"\"\"\n# TODO caution we should ensure that no handler method (tool) returns simply\n# an empty string (e.g when showing contents of an empty file), since that\n# would be considered an invalid response, and other responders will wrongly\n# be given a chance to respond.\nreturn (\nresult is not None\nand (result.content != \"\" or result.function_call is not None)\nand (  # if NO_ANSWER is from controller, then it means\n# controller is stuck and we are done with task loop\nNO_ANSWER not in result.content\nor result.metadata.sender == self.controller\n)\n)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\nself,\nresp: Responder,\nmsg: ChatDocument | None = None,\nmark: bool = False,\n) -&gt; None:\n\"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\ndefault_values = ChatDocLoggerFields().dict().values()\nmsg_str_tsv = \"\\t\".join(str(v) for v in default_values)\nif msg is not None:\nmsg_str_tsv = msg.tsv_str()\nmark_str = \"*\" if mark else \" \"\ntask_name = self.name if self.name != \"\" else \"root\"\nresp_color = \"white\" if mark else \"red\"\nresp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\nif msg is None:\nmsg_str = f\"{mark_str}({task_name}) {resp_str}\"\nelse:\ncolor = {\nEntity.LLM: \"green\",\nEntity.USER: \"blue\",\nEntity.AGENT: \"red\",\n}[msg.metadata.sender]\nf = msg.log_fields()\ntool_type = f.tool_type.rjust(6)\ntool_name = f.tool.rjust(10)\ntool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\nsender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\nsender_name = f.sender_name.rjust(10)\nrecipient = \"=&gt;\" + str(f.recipient).rjust(10)\nblock = \"X \" + str(f.block or \"\").rjust(10)\ncontent = f\"[{color}]{f.content}[/{color}]\"\nmsg_str = (\nf\"{mark_str}({task_name}) \"\nf\"{resp_str} {sender}({sender_name}) \"\nf\"({recipient}) ({block}) {tool_str} {content}\"\n)\nif self.logger is not None:\nself.logger.log(msg_str)\nif self.tsv_logger is not None:\nresp_str = str(resp)\nself.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/agent/tool_message/","title":"tool_message","text":"<p>langroid/agent/tool_message.py </p> <p>Structured messages to an agent, typically from an LLM, to be handled by an agent. The messages could represent, for example: - information or data given to the agent - request for information or data from the agent - request to run a method of the agent</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>             Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently these \"tools\" are handled by methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef examples(cls) -&gt; List[\"ToolMessage\"]:\n\"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Returns:\n    \"\"\"\nraise NotImplementedError(\"Subclass must implement this method\")\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.usage_example","title":"<code>usage_example()</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing an example of how to use the message.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>example of how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_example(cls) -&gt; str:\n\"\"\"\n    Instruction to the LLM showing an example of how to use the message.\n    Returns:\n        str: example of how to use the message\n    \"\"\"\n# pick a random example of the fields\nex = choice(cls.examples())\nreturn ex.json_example()\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>field name</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>default value of the field</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n\"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n    Returns:\n        str: default value of the field\n    \"\"\"\nschema = cls.schema()\nproperties = schema[\"properties\"]\nreturn properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.llm_function_schema","title":"<code>llm_function_schema()</code>  <code>classmethod</code>","text":"<p>Returns schema for use in OpenAI Function Calling API.</p> <p>Returns:</p> Type Description <code>LLMFunctionSpec</code> <p>Dict[str, Any]: schema for use in OpenAI Function Calling API</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(cls) -&gt; LLMFunctionSpec:\n\"\"\"\n    Returns schema for use in OpenAI Function Calling API.\n    Returns:\n        Dict[str, Any]: schema for use in OpenAI Function Calling API\n    \"\"\"\nschema = cls.schema()\nspec = LLMFunctionSpec(\nname=cls.default_value(\"request\"),\ndescription=cls.default_value(\"purpose\"),\nparameters=dict(),\n)\nexcludes = [\"result\", \"request\", \"purpose\"]\nproperties = {}\nif schema.get(\"properties\"):\nproperties = {\nfield: details\nfor field, details in schema[\"properties\"].items()\nif field not in excludes\n}\nrequired = []\nif schema.get(\"required\"):\nrequired = [field for field in schema[\"required\"] if field not in excludes]\nproperties = {\nk: {prop: val for prop, val in v.items() if prop != \"title\"}\nfor k, v in properties.items()\n}\nspec.parameters = dict(\ntype=\"object\",\nproperties=properties,\nrequired=required,\n)\nreturn spec\n</code></pre>"},{"location":"reference/agent/special/","title":"special","text":"<p>langroid/agent/special/init.py </p>"},{"location":"reference/agent/special/doc_chat_agent/","title":"doc_chat_agent","text":"<p>langroid/agent/special/doc_chat_agent.py </p> <p>Agent that supports asking queries about a set of documents, using retrieval-augmented queries. Functionality includes: - summarizing a document, with a custom instruction; see <code>summarize_docs</code> - asking a question about a document; see <code>answer_from_docs</code></p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgentConfig","title":"<code>DocChatAgentConfig</code>","text":"<p>             Bases: <code>ChatAgentConfig</code></p> <p>Attributes:</p> Name Type Description <code>max_context_tokens</code> <code>int</code> <p>threshold to use for various steps, e.g. if we are able to fit the current stage of doc processing into this many tokens, we skip additional compression steps, and use the current docs as-is in the context</p> <code>conversation_mode</code> <code>bool</code> <p>if True, we will accumulate message history, and pass entire history to LLM at each round. If False, each request to LLM will consist only of the initial task messages plus the current query.</p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent","title":"<code>DocChatAgent(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def __init__(\nself,\nconfig: DocChatAgentConfig,\n):\nsuper().__init__(config)\nself.config: DocChatAgentConfig = config\nself.original_docs: None | List[Document] = None\nself.original_docs_length = 0\nself.response: None | Document = None\nif len(config.doc_paths) &gt; 0:\nself.ingest()\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> <p>Returns:</p> Type Description <code>None</code> <p>dict with keys: n_splits: number of splits urls: list of urls paths: list of file paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n\"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n    Returns:\n        dict with keys:\n            n_splits: number of splits\n            urls: list of urls\n            paths: list of file paths\n    \"\"\"\nif len(self.config.doc_paths) == 0:\nreturn\nurls, paths = get_urls_and_paths(self.config.doc_paths)\ndocs: List[Document] = []\nif len(urls) &gt; 0:\nloader = URLLoader(urls=urls)\ndocs = loader.load()\nif len(paths) &gt; 0:\nfor p in paths:\npath_docs = RepoLoader.get_documents(p)\ndocs.extend(path_docs)\nn_docs = len(docs)\nn_splits = self.ingest_docs(docs)\nif n_docs == 0:\nreturn\nn_urls = len(urls)\nn_paths = len(paths)\nprint(\nf\"\"\"\n    [green]I have processed the following {n_urls} URLs \n    and {n_paths} paths into {n_splits} parts:\n    \"\"\".strip()\n)\nprint(\"\\n\".join(urls))\nprint(\"\\n\".join(paths))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs)</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(self, docs: List[Document]) -&gt; int:\n\"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n    \"\"\"\nself.original_docs = docs\nif self.parser is None:\nraise ValueError(\"Parser not set\")\ndocs = self.parser.split(docs)\nif self.vecdb is None:\nraise ValueError(\"VecDB not set\")\nself.vecdb.add_documents(docs)\nself.original_docs_length = self.doc_length(docs)\nreturn len(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n\"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\nif self.parser is None:\nraise ValueError(\"Parser not set\")\nreturn self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n\"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\ncontents = [f\"Extract: {d.content}\" for d in docs]\nsources = [d.metadata.source for d in docs]\nsources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\nreturn \"\\n\".join(\n[\nf\"\"\"\n{content}\n{source}\n            \"\"\"\nfor (content, source) in zip(contents, sources)\n]\n)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to answer</p> required <code>passages</code> <code>List[Document]</code> <p>list of <code>Document</code> objects each containing a possibly relevant snippet, and metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the answer,</p> <code>Document</code> <p>and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n    \"\"\"\npassages_str = self.doc_string(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = self.config.summarize_prompt.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt.\n# Note this will send entire message history, plus this final_prompt\n# to the LLM, and self.message_history will be updated to include\n# 2 new LLMMessage objects:\n# one for `final_prompt`, and one for the LLM response\n# TODO need to \"forget\" last two messages in message_history\n# if we are not in conversation mode\nif self.config.conversation_mode:\n# respond with temporary context\nanswer_doc = super()._llm_response_temp_context(question, final_prompt)\nelse:\nanswer_doc = super().llm_response_forget(final_prompt)\nfinal_answer = answer_doc.content.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata=DocMetaData(\nsource=\"SOURCE: \" + sources,\nsender=Entity.LLM,\ncached=getattr(answer_doc.metadata, \"cached\", False),\n),\n)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on docs in vecdb, and conv history</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef answer_from_docs(self, query: str) -&gt; Document:\n\"\"\"Answer query based on docs in vecdb, and conv history\"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif len(self.dialog) &gt; 0 and not self.config.conversation_mode:\n# In conversation mode, we let self.message_history accumulate\n# and do not need to convert to standalone query\n# (We rely on the LLM to interpret the new query in the context of\n# the message history so far)\nwith console.status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\nwith StreamingIfAllowed(self.llm, False):\nquery = self.llm.followup_to_standalone(self.dialog, query)\nprint(f\"[orange2]New query: {query}\")\npassages = self.original_docs\n# if original docs too long, no need to look for relevant parts.\nif (\npassages is None\nor self.original_docs_length &gt; self.config.max_context_tokens\n):\nwith console.status(\"[cyan]Searching VecDB for relevant doc passages...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\nif len(docs_and_scores) == 0:\nreturn response\npassages = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\n# if passages not too long, no need to extract relevant verbatim text\nextracts = passages\nif self.doc_length(passages) &gt; self.config.max_context_tokens:\nwith console.status(\"[cyan]LLM Extracting verbatim passages...\"):\nwith StreamingIfAllowed(self.llm, False):\nextracts = self.llm.get_verbatim_extracts(query, passages)\nwith ExitStack() as stack:\n# conditionally use Streaming or rich console context\ncm = (\nStreamingIfAllowed(self.llm)\nif settings.stream\nelse (console.status(\"LLM Generating final answer...\"))\n)\nstack.enter_context(cm)\nresponse = self.get_summary_answer(query, extracts)\nself.update_dialog(query, response.content)\nself.response = response  # save last response\nreturn response\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\nself,\ninstruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n\"\"\"Summarize all docs\"\"\"\nif self.original_docs is None:\nlogger.warning(\n\"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection? \n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs. \n            \"\"\"\n)\nreturn None\nfull_text = \"\\n\\n\".join([d.content for d in self.original_docs])\nif self.parser is None:\nraise ValueError(\"No parser defined\")\ntot_tokens = self.parser.num_tokens(full_text)\nmodel = (\nself.config.llm.chat_model\nif self.config.llm.use_chat_for_completion\nelse self.config.llm.completion_model\n)\nMAX_INPUT_TOKENS = (\nself.config.llm.context_length[model]\n- self.config.llm.max_output_tokens\n- 100\n)\nif tot_tokens &gt; MAX_INPUT_TOKENS:\n# truncate\nfull_text = self.parser.tokenizer.decode(\nself.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n)\nlogger.warning(\nf\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n)\nprompt = f\"\"\"\n{instruction}\n{full_text}\n    \"\"\".strip()\nwith StreamingIfAllowed(self.llm):  # type: ignore\nsummary = Agent.llm_response(self, prompt)\nreturn summary  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; None:\n\"\"\"Show evidence for last response\"\"\"\nif self.response is None:\nprint(\"[magenta]No response yet\")\nreturn\nsource = self.response.metadata.source\nif len(source) &gt; 0:\nprint(\"[magenta]\" + source)\nelse:\nprint(\"[magenta]No source found\")\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/","title":"recipient_validator_agent","text":"<p>langroid/agent/special/recipient_validator_agent.py </p>"},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator","title":"<code>RecipientValidator(config)</code>","text":"<p>             Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>def __init__(self, config: RecipientValidatorConfig):\nsuper().__init__(config)\nself.config: RecipientValidatorConfig = config\nself.llm = None\nself.vecdb = None\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Check whether the incoming message is in the expected format. Used to check whether the output of the LLM of the calling agent is in the expected format.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the incoming message (pending message of the task)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]:</p> <code>Optional[ChatDocument]</code> <ul> <li>if msg is in expected format, return None (no objections)</li> </ul> <code>Optional[ChatDocument]</code> <ul> <li>otherwise, a ChatDocument that either contains a request to LLM to clarify/fix the msg, or a fixed version of the LLM's original message.</li> </ul> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>def agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Check whether the incoming message is in the expected format.\n    Used to check whether the output of the LLM of the calling agent is\n    in the expected format.\n    Args:\n        msg (str|ChatDocument): the incoming message (pending message of the task)\n    Returns:\n        Optional[ChatDocument]:\n        - if msg is in expected format, return None (no objections)\n        - otherwise, a ChatDocument that either contains a request to\n            LLM to clarify/fix the msg, or a fixed version of the LLM's original\n            message.\n    \"\"\"\nif msg is None:\nreturn None\nif isinstance(msg, str):\nmsg = ChatDocument.from_str(msg)\nrecipient = msg.metadata.recipient\nhas_func_call = msg.function_call is not None\ncontent = msg.content\nif recipient != \"\":\n# there is a clear recipient, return None (no objections)\nreturn None\nattachment: None | ChatDocAttachment = None\nresponder: None | Entity = None\nsender_name = self.config.name\nif (\nhas_func_call or \"TOOL\" in content\n) and self.config.tool_recipient is not None:\n# assume it is meant for Coder, so simply set the recipient field,\n# and the parent task loop continues as normal\n# TODO- but what if it is not a legit function call\nrecipient = self.config.tool_recipient\nelif content in self.config.recipients:\n# the incoming message is a clarification response from LLM\nrecipient = content\nif msg.attachment is not None and isinstance(\nmsg.attachment, RecipientValidatorAttachment\n):\ncontent = msg.attachment.content\nelse:\nlogger.warning(\"ValidatorAgent: Did not find content to correct\")\ncontent = \"\"\n# we've used the attachment, don't need anymore\nattachment = RecipientValidatorAttachment(content=\"\")\n# we are rewriting an LLM message from parent, so\n# pretend it is from LLM\nresponder = Entity.LLM\nsender_name = \"\"\nelse:\n# save the original message so when the Validator\n# receives the LLM clarification,\n# it can use it as the `content` field\nattachment = RecipientValidatorAttachment(content=content)\nrecipient_str = \", \".join(self.config.recipients)\ncontent = f\"\"\"\n        Who is this message for? \n        Please simply respond with one of these names:\n{recipient_str}\n        \"\"\"\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Validator: {content}\")\nreturn ChatDocument(\ncontent=content,\nfunction_call=msg.function_call if has_func_call else None,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nparent_responder=responder,\nsender_name=sender_name,\nrecipient=recipient,\n),\n)\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/","title":"retriever_agent","text":"<p>langroid/agent/special/retriever_agent.py </p> <p>Agent to retrieve relevant verbatim whole docs/records from a vector store.</p>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent","title":"<code>RetrieverAgent(config)</code>","text":"<p>             Bases: <code>DocChatAgent</code>, <code>ABC</code></p> <p>Agent for retrieving whole records/docs matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def __init__(self, config: RetrieverAgentConfig):\nsuper().__init__(config)\nself.config: RetrieverAgentConfig = config\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.get_nearest_docs","title":"<code>get_nearest_docs(query)</code>","text":"<p>Given a query, get the records/docs whose contents are closest to the     query, in terms of vector similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def get_nearest_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n    Given a query, get the records/docs whose contents are closest to the\n        query, in terms of vector similarity.\n    Args:\n        query: query string\n    Returns:\n        list of Document objects\n    \"\"\"\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn []\nwith console.status(\"[cyan]Searching VecDB for similar docs/records...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\ndocs: List[Document] = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\nreturn docs\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.get_relevant_docs","title":"<code>get_relevant_docs(query)</code>","text":"<p>Given a query, get the records/docs whose contents are most relevant to the     query. First get nearest docs from vector store, then select the best     matches according to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def get_relevant_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n    Given a query, get the records/docs whose contents are most relevant to the\n        query. First get nearest docs from vector store, then select the best\n        matches according to the LLM.\n    Args:\n        query (str): query string\n    Returns:\n        List[Document]: list of Document objects\n    \"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nnearest_docs = self.get_nearest_docs(query)\nif len(nearest_docs) == 0:\nreturn [response]\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn nearest_docs\nwith console.status(\"LLM selecting relevant docs from retrieved ones...\"):\ndoc_list = self.llm_select_relevant_docs(query, nearest_docs)\nreturn doc_list\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.llm_select_relevant_docs","title":"<code>llm_select_relevant_docs(query, docs)</code>","text":"<p>Given a query and a list of docs, select the docs whose contents match best,     according to the LLM. Use the doc IDs to select the docs from the vector     store.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def llm_select_relevant_docs(\nself, query: str, docs: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n    Given a query and a list of docs, select the docs whose contents match best,\n        according to the LLM. Use the doc IDs to select the docs from the vector\n        store.\n    Args:\n        query: query string\n        docs: list of Document objects\n    Returns:\n        list of Document objects\n    \"\"\"\ndoc_contents = \"\\n\\n\".join(\n[f\"DOC: ID={d.id()}, content={d.content}\" for d in docs]\n)\nprompt = f\"\"\"\n    Given the following QUERY: \n{query}\n    and the following DOCS with IDs and contents\n{doc_contents}\n    Find at most {self.config.n_matches} DOCs that are most relevant to the QUERY.\n    Return your as a sequence of DOC IDS ONLY, for example: \n    \"id1 id2 id3...\"\n    \"\"\"\ndefault_response = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn [default_response]\nresponse = self.llm.generate(  # type: ignore\nprompt, max_tokens=self.config.llm.max_output_tokens  # type: ignore\n)\nif response.message == NO_ANSWER:\nreturn [default_response]\nids = response.message.split()\nif len(ids) == 0:\nreturn [default_response]\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn [default_response]\ndocs = self.vecdb.get_documents_by_ids(ids)\nreturn [\nDocument(content=d.content, metadata=DocMetaData(source=\"LLM\"))\nfor d in docs\n]\n</code></pre>"},{"location":"reference/cachedb/","title":"cachedb","text":"<p>langroid/cachedb/init.py </p>"},{"location":"reference/cachedb/base/","title":"base","text":"<p>langroid/cachedb/base.py </p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB","title":"<code>CacheDB</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a cache database.</p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.store","title":"<code>store(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>dict</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef store(self, key: str, value: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Abstract method to store a value associated with a key.\n    Args:\n        key (str): The key under which to store the value.\n        value (dict): The value to store.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.retrieve","title":"<code>retrieve(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n    Abstract method to retrieve the value associated with a key.\n    Args:\n        key (str): The key to retrieve the value for.\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/","title":"momento_cachedb","text":"<p>langroid/cachedb/momento_cachedb.py </p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCacheConfig","title":"<code>MomentoCacheConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration model for RedisCache.</p>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache","title":"<code>MomentoCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Momento implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MomentoCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def __init__(self, config: MomentoCacheConfig):\n\"\"\"\n    Initialize a MomentoCache with the given config.\n    Args:\n        config (MomentoCacheConfig): The configuration to use.\n    \"\"\"\nself.config = config\nload_dotenv()\nmomento_token = os.getenv(\"MOMENTO_AUTH_TOKEN\")\nif momento_token is None:\nraise ValueError(\"\"\"MOMENTO_AUTH_TOKEN not set in .env file\"\"\")\nelse:\nself.client = momento.CacheClient(\nconfiguration=momento.Configurations.Laptop.v1(),\ncredential_provider=momento.CredentialProvider.from_environment_variable(\n\"MOMENTO_AUTH_TOKEN\"\n),\ndefault_ttl=timedelta(seconds=self.config.ttl),\n)\nself.client.create_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear keys from current db.\"\"\"\nself.client.flush_cache(self.config.cachename)\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n\"\"\"\n    Store a value associated with a key.\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\nself.client.set(self.config.cachename, key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/momento_cachedb/#langroid.cachedb.momento_cachedb.MomentoCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/momento_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n    Retrieve the value associated with a key.\n    Args:\n        key (str): The key to retrieve the value for.\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\nvalue = self.client.get(self.config.cachename, key)\nif isinstance(value, CacheGet.Hit):\nreturn json.loads(value.value_string)  # type: ignore\nelse:\nreturn None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/","title":"redis_cachedb","text":"<p>langroid/cachedb/redis_cachedb.py </p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Configuration model for RedisCache.</p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache","title":"<code>RedisCache(config)</code>","text":"<p>             Bases: <code>CacheDB</code></p> <p>Redis implementation of the CacheDB.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def __init__(self, config: RedisCacheConfig):\n\"\"\"\n    Initialize a RedisCache with the given config.\n    Args:\n        config (RedisCacheConfig): The configuration to use.\n    \"\"\"\nself.config = config\nload_dotenv()\nif self.config.fake:\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nredis_password = os.getenv(\"REDIS_PASSWORD\")\nredis_host = os.getenv(\"REDIS_HOST\")\nredis_port = os.getenv(\"REDIS_PORT\")\nif None in [redis_password, redis_host, redis_port]:\nlogger.warning(\n\"\"\"REDIS_PASSWORD, REDIS_HOST, REDIS_PORT not set in .env file,\n                using fake redis client\"\"\"\n)\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nself.client = redis.Redis(  # type: ignore\nhost=redis_host,\nport=redis_port,\npassword=redis_password,\n)\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear keys from current db.\"\"\"\nself.client.flushdb()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all keys from all dbs.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear_all(self) -&gt; None:\n\"\"\"Clear all keys from all dbs.\"\"\"\nself.client.flushall()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n\"\"\"\n    Store a value associated with a key.\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\nself.client.set(key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n    Retrieve the value associated with a key.\n    Args:\n        key (str): The key to retrieve the value for.\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\nvalue = self.client.get(key)\nreturn json.loads(value) if value else None\n</code></pre>"},{"location":"reference/embedding_models/","title":"embedding_models","text":"<p>langroid/embedding_models/init.py </p>"},{"location":"reference/embedding_models/base/","title":"base","text":"<p>langroid/embedding_models/base.py </p>"},{"location":"reference/embedding_models/base/#langroid.embedding_models.base.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for an embedding model.</p>"},{"location":"reference/embedding_models/models/","title":"models","text":"<p>langroid/embedding_models/models.py </p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>\"openai\" or \"sentencetransformer\" # others soon</p> <code>'openai'</code> <p>Returns:</p> Type Description <code>EmbeddingModel</code> <p>EmbeddingModel</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n\"\"\"\n    Args:\n        embedding_fn_type: \"openai\" or \"sentencetransformer\" # others soon\n    Returns:\n        EmbeddingModel\n    \"\"\"\nif embedding_fn_type == \"openai\":\nreturn OpenAIEmbeddings  # type: ignore\nelse:  # default sentence transformer\nreturn SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/language_models/","title":"language_models","text":"<p>langroid/language_models/init.py </p>"},{"location":"reference/language_models/base/","title":"base","text":"<p>langroid/language_models/base.py </p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicate it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing message sent to, or received from, LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage.api_dict","title":"<code>api_dict()</code>","text":"<p>Convert to dictionary for API request.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self) -&gt; Dict[str, Any]:\n\"\"\"\n    Convert to dictionary for API request.\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\nd = self.dict()\n# drop None values since API doesn't accept them\ndict_no_none = {k: v for k, v in d.items() if v is not None}\nif \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n# OpenAI API does not like empty name\ndel dict_no_none[\"name\"]\nif \"function_call\" in dict_no_none:\n# arguments must be a string\nif \"arguments\" in dict_no_none[\"function_call\"]:\ndict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\ndict_no_none[\"function_call\"][\"arguments\"]\n)\nreturn dict_no_none\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Class representing response from LLM.</p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.recipient_message","title":"<code>recipient_message()</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains \"TO:  \", or (b) <code>message</code> is empty and <code>function_call</code> with <code>to: &lt;name&gt;</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def recipient_message(\nself,\n) -&gt; Tuple[str, str]:\n\"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n    Two cases:\n    (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n    (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n    \"\"\"\nif self.function_call is not None:\nreturn self.function_call.to, \"\"\nelse:\nmsg = self.message\nrecipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\nreturn recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel","title":"<code>LanguageModel(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, config: LLMConfig):\nself.config = config\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.create","title":"<code>create(config)</code>  <code>staticmethod</code>","text":"<p>Create a language model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[LLMConfig]</code> <p>configuration for language model</p> required Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[Type[\"LanguageModel\"]]:\n\"\"\"\n    Create a language model.\n    Args:\n        config: configuration for language model\n    Returns: instance of language model\n    \"\"\"\nfrom langroid.language_models.openai_gpt import OpenAIGPT\nif config is None or config.type is None:\nreturn None\ncls = dict(\nopenai=OpenAIGPT,\n).get(config.type, OpenAIGPT)\nreturn cls(config)  # type: ignore\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.set_stream","title":"<code>set_stream(stream)</code>  <code>abstractmethod</code>","text":"<p>Enable or disable streaming output from API. Return previous value of stream.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n    Return previous value of stream.\"\"\"\npass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\npass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.followup_to_standalone","title":"<code>followup_to_standalone(chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>List[Tuple[str, str]]</code> <p>list of tuples of (question, answer)</p> required <code>query</code> <p>follow-up question</p> required Source code in <code>langroid/language_models/base.py</code> <pre><code>def followup_to_standalone(\nself, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n\"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n    Returns: standalone version of the question\n    \"\"\"\nhistory = collate_chat_history(chat_history)\nprompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n    Chat history: {history}\n    Follow-up question: {question}     \"\"\".strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\nstandalone = self.generate(prompt=prompt, max_tokens=1024).message.strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\nreturn standalone\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question. Asynch allows parallel calls to the LLM API.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>async def get_verbatim_extract_async(self, question: str, passage: Document) -&gt; str:\n\"\"\"\n    Asynchronously, get verbatim extract from passage\n    that is relevant to a question.\n    Asynch allows parallel calls to the LLM API.\n    \"\"\"\nasync with aiohttp.ClientSession():\ntemplatized_prompt = EXTRACTION_PROMPT_GPT4\nfinal_prompt = templatized_prompt.format(\nquestion=question, content=passage.content\n)\nshow_if_debug(final_prompt, \"EXTRACT-PROMPT= \")\nfinal_extract = await self.agenerate(prompt=final_prompt, max_tokens=1024)\nshow_if_debug(final_extract.message.strip(), \"EXTRACT-RESPONSE= \")\nreturn final_extract.message.strip()\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to be answered</p> required <code>passages</code> <code>List[Document]</code> <p>list of passages from which to extract relevant verbatim text</p> required <code>LLM</code> <p>LanguageModel to use for generating the prompt and extract</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of verbatim extracts from passages that are relevant to question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_verbatim_extracts(\nself, question: str, passages: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts from passages that are relevant to question\n    \"\"\"\ndocs = asyncio.run(self._get_verbatim_extracts(question, passages))\nreturn docs\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to answer</p> required <code>passages</code> <code>List[Document]</code> <p>list of <code>Document</code> objects each containing a possibly relevant snippet, and metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the answer,</p> <code>Document</code> <p>and metadata containing source citations</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n    \"\"\"\n# Define an auxiliary function to transform the list of\n# passages into a single string\ndef stringify_passages(passages: List[Document]) -&gt; str:\nreturn \"\\n\".join(\n[\nf\"\"\"\n            Extract: {p.content}\n            Source: {p.metadata.source}\n            \"\"\"\nfor p in passages\n]\n)\npassages_str = stringify_passages(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = SUMMARY_ANSWER_PROMPT_GPT4.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt\nllm_response = self.generate(prompt=final_prompt, max_tokens=1024)\nfinal_answer = llm_response.message.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata={\"source\": \"SOURCE: \" + sources, \"cached\": llm_response.cached},\n)\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.StreamingIfAllowed","title":"<code>StreamingIfAllowed(llm, stream=True)</code>","text":"<p>Context to temporarily enable or disable streaming, if allowed globally via <code>settings.stream</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def __init__(self, llm: LanguageModel, stream: bool = True):\nself.llm = llm\nself.stream = stream\n</code></pre>"},{"location":"reference/language_models/openai_gpt/","title":"openai_gpt","text":"<p>langroid/language_models/openai_gpt.py </p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Chat models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIResponse","title":"<code>OpenAIResponse</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>OpenAI response model, either completion or chat.</p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT","title":"<code>OpenAIGPT(config)</code>","text":"<p>             Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig):\n\"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\nsuper().__init__(config)\nif settings.nofunc:\nself.chat_model = OpenAIChatModel.GPT4_NOFUNC\nload_dotenv()\nself.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\nif self.api_key == \"\":\nraise ValueError(\n\"\"\"\n            OPENAI_API_KEY not set in .env file,\n            please set it to your OpenAI API key.\"\"\"\n)\nself.cache: MomentoCache | RedisCache\nif settings.cache_type == \"momento\":\nconfig.cache_config = MomentoCacheConfig()\nself.cache = MomentoCache(config.cache_config)\nelse:\nconfig.cache_config = RedisCacheConfig()\nself.cache = RedisCache(config.cache_config)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>enable streaming output from API</p> required Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\ntmp = self.config.stream\nself.config.stream = stream\nreturn tmp\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\nreturn self.config.stream\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat","title":"<code>chat(messages, max_tokens, functions=None, function_call='auto')</code>","text":"<p>ChatCompletion API call to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[LLMMessage]]</code> <p>list of messages  to send to the API, typically represents back and forth dialogue between user and LLM, but could also include \"function\"-role messages. If messages is a string, it is assumed to be a user message.</p> required <code>max_tokens</code> <code>int</code> <p>max output tokens to generate</p> required <code>functions</code> <code>Optional[List[LLMFunctionSpec]]</code> <p>list of LLMFunction specs available to the LLM, to possibly use in its response</p> <code>None</code> <code>function_call</code> <code>str | Dict[str, str]</code> <p>controls how the LLM uses <code>functions</code>: - \"auto\": LLM decides whether to use <code>functions</code> or not, - \"none\": LLM blocked from using any function - a dict of {\"name\": \"function_name\"} which forces the LLM to use     the specified function.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse object</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat(\nself,\nmessages: Union[str, List[LLMMessage]],\nmax_tokens: int,\nfunctions: Optional[List[LLMFunctionSpec]] = None,\nfunction_call: str | Dict[str, str] = \"auto\",\n) -&gt; LLMResponse:\n\"\"\"\n    ChatCompletion API call to OpenAI.\n    Args:\n        messages: list of messages  to send to the API, typically\n            represents back and forth dialogue between user and LLM, but could\n            also include \"function\"-role messages. If messages is a string,\n            it is assumed to be a user message.\n        max_tokens: max output tokens to generate\n        functions: list of LLMFunction specs available to the LLM, to possibly\n            use in its response\n        function_call: controls how the LLM uses `functions`:\n            - \"auto\": LLM decides whether to use `functions` or not,\n            - \"none\": LLM blocked from using any function\n            - a dict of {\"name\": \"function_name\"} which forces the LLM to use\n                the specified function.\n    Returns:\n        LLMResponse object\n    \"\"\"\nopenai.api_key = self.api_key\nif type(messages) == str:\nllm_messages = [\nLLMMessage(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\nLLMMessage(role=Role.USER, content=messages),\n]\nelse:\nllm_messages = cast(List[LLMMessage], messages)\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):  # type: ignore\ncached = False\nhashed_key, result = self._cache_lookup(\"Completion\", **kwargs)\nif result is not None:\ncached = True\nif settings.debug:\nprint(\"[red]CACHED[/red]\")\nelse:\n# If it's not in the cache, call the API\nresult = openai.ChatCompletion.create(**kwargs)  # type: ignore\nif not self.config.stream:\n# if streaming, cannot cache result\n# since it is a generator. Instead,\n# we hold on to the hashed_key and\n# cache the result later\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\nargs: Dict[str, Any] = dict(\nmodel=self.config.chat_model,\nmessages=[m.api_dict() for m in llm_messages],\nmax_tokens=max_tokens,\nn=1,\nstop=None,\ntemperature=0.5,\nrequest_timeout=self.config.timeout,\nstream=self.config.stream,\n)\n# only include functions-related args if functions are provided\n# since the OpenAI API will throw an error if `functions` is None or []\nif functions is not None:\nargs.update(\ndict(\nfunctions=[f.dict() for f in functions],\nfunction_call=function_call,\n)\n)\ncached, hashed_key, response = completions_with_backoff(**args)\nif self.config.stream and not cached:\nllm_response, openai_response = self._stream_response(response, chat=True)\nself.cache.store(hashed_key, openai_response)\nreturn llm_response\nusage = response[\"usage\"][\"total_tokens\"]\n# openAI response will look like this:\n\"\"\"\n    {\n        \"id\": \"chatcmpl-123\",\n        \"object\": \"chat.completion\",\n        \"created\": 1677652288,\n        \"choices\": [{\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"name\": \"\", \n                \"content\": \"\\n\\nHello there, how may I help you?\",\n                \"function_call\": {\n                    \"name\": \"fun_name\",\n                    \"arguments: {\n                        \"arg1\": \"val1\",\n                        \"arg2\": \"val2\"\n                    }\n                }, \n            },\n            \"finish_reason\": \"stop\"\n        }],\n        \"usage\": {\n            \"prompt_tokens\": 9,\n            \"completion_tokens\": 12,\n            \"total_tokens\": 21\n        }\n    }\n    \"\"\"\nmessage = response[\"choices\"][0][\"message\"]\nmsg = message[\"content\"] or \"\"\nif message.get(\"function_call\") is None:\nfun_call = None\nelse:\nfun_call = LLMFunctionCall(name=message[\"function_call\"][\"name\"])\ntry:\nfun_args = ast.literal_eval(message[\"function_call\"][\"arguments\"])\nfun_call.arguments = fun_args\nexcept (ValueError, SyntaxError):\nlogging.warning(\nf\"Could not parse function arguments: \"\nf\"{message['function_call']['arguments']} \"\nf\"for function {message['function_call']['name']} \"\nf\"treating as normal non-function message\"\n)\nfun_call = None\nmsg = message[\"content\"] + message[\"function_call\"][\"arguments\"]\nreturn LLMResponse(\nmessage=msg.strip() if msg is not None else \"\",\nfunction_call=fun_call,\nusage=usage,\ncached=cached,\n)\n</code></pre>"},{"location":"reference/language_models/utils/","title":"utils","text":"<p>langroid/language_models/utils.py </p>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=(requests.exceptions.RequestException, openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def retry_with_exponential_backoff(\nfunc: Callable[..., Any],\ninitial_delay: float = 1,\nexponential_base: float = 2,\njitter: bool = True,\nmax_retries: int = 10,\nerrors: tuple = (  # type: ignore\nrequests.exceptions.RequestException,\nopenai.error.Timeout,\nopenai.error.RateLimitError,\nopenai.error.APIError,\nopenai.error.ServiceUnavailableError,\nopenai.error.TryAgain,\naiohttp.ServerTimeoutError,\nasyncio.TimeoutError,\n),\n) -&gt; Callable[..., Any]:\n\"\"\"Retry a function with exponential backoff.\"\"\"\ndef wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n# Initialize variables\nnum_retries = 0\ndelay = initial_delay\n# Loop until a successful response or max_retries is hit or exception is raised\nwhile True:\ntry:\nreturn func(*args, **kwargs)\nexcept openai.error.InvalidRequestError as e:\n# do not retry when the request itself is invalid,\n# e.g. when context is too long\nlogger.error(f\"OpenAI API request failed with error: {e}.\")\nraise e\n# Retry on specified errors\nexcept errors as e:\n# Increment retries\nnum_retries += 1\n# Check if max retries has been reached\nif num_retries &gt; max_retries:\nraise Exception(\nf\"Maximum number of retries ({max_retries}) exceeded.\"\n)\n# Increment the delay\ndelay *= exponential_base * (1 + jitter * random.random())\nlogger.warning(\nf\"\"\"OpenAI API request failed with error: \n{e}. \n                    Retrying in {delay} seconds...\"\"\"\n)\n# Sleep for the delay\ntime.sleep(delay)\n# Raise exceptions for any errors not specified\nexcept Exception as e:\nraise e\nreturn wrapper\n</code></pre>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.async_retry_with_exponential_backoff","title":"<code>async_retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=(openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError))</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def async_retry_with_exponential_backoff(\nfunc: Callable[..., Any],\ninitial_delay: float = 1,\nexponential_base: float = 2,\njitter: bool = True,\nmax_retries: int = 10,\nerrors: tuple = (  # type: ignore\nopenai.error.Timeout,\nopenai.error.RateLimitError,\nopenai.error.APIError,\nopenai.error.ServiceUnavailableError,\nopenai.error.TryAgain,\naiohttp.ServerTimeoutError,\nasyncio.TimeoutError,\n),\n) -&gt; Callable[..., Any]:\n\"\"\"Retry a function with exponential backoff.\"\"\"\nasync def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n# Initialize variables\nnum_retries = 0\ndelay = initial_delay\n# Loop until a successful response or max_retries is hit or exception is raised\nwhile True:\ntry:\nresult = await func(*args, **kwargs)\nreturn result\nexcept openai.error.InvalidRequestError as e:\n# do not retry when the request itself is invalid,\n# e.g. when context is too long\nlogger.error(f\"OpenAI API request failed with error: {e}.\")\nraise e\n# Retry on specified errors\nexcept errors as e:\n# Increment retries\nnum_retries += 1\n# Check if max retries has been reached\nif num_retries &gt; max_retries:\nraise Exception(\nf\"Maximum number of retries ({max_retries}) exceeded.\"\n)\n# Increment the delay\ndelay *= exponential_base * (1 + jitter * random.random())\nlogger.warning(\nf\"\"\"OpenAI API request failed with error{e}. \n                    Retrying in {delay} seconds...\"\"\"\n)\n# Sleep for the delay\ntime.sleep(delay)\n# Raise exceptions for any errors not specified\nexcept Exception as e:\nraise e\nreturn wrapper\n</code></pre>"},{"location":"reference/parsing/","title":"parsing","text":"<p>langroid/parsing/init.py </p>"},{"location":"reference/parsing/agent_chats/","title":"agent_chats","text":"<p>langroid/parsing/agent_chats.py </p>"},{"location":"reference/parsing/agent_chats/#langroid.parsing.agent_chats.parse_message","title":"<code>parse_message(msg)</code>","text":"<p>Parse the intended recipient and content of a message. Message format is assumed to be TO[]:. The TO[]: part is optional. <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to parse</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>str, str: task-name of intended recipient, and content of message (if recipient is not specified, task-name is empty string)</p> Source code in <code>langroid/parsing/agent_chats.py</code> <pre><code>@no_type_check\ndef parse_message(msg: str) -&gt; Tuple[str, str]:\n\"\"\"\n    Parse the intended recipient and content of a message.\n    Message format is assumed to be TO[&lt;recipient&gt;]:&lt;message&gt;.\n    The TO[&lt;recipient&gt;]: part is optional.\n    Args:\n        msg (str): message to parse\n    Returns:\n        str, str: task-name of intended recipient, and content of message\n            (if recipient is not specified, task-name is empty string)\n    \"\"\"\nif msg is None:\nreturn \"\", \"\"\n# Grammar definition\nname = Word(alphanums)\nto_start = Literal(\"TO[\").suppress()\nto_end = Literal(\"]:\").suppress()\nto_field = (to_start + name(\"name\") + to_end) | Empty().suppress()\nmessage = SkipTo(StringEnd())(\"text\")\n# Parser definition\nparser = to_field + message\ntry:\nparsed = parser.parseString(msg)\nreturn parsed.name, parsed.text\nexcept ParseException:\nreturn \"\", msg\n</code></pre>"},{"location":"reference/parsing/code_parser/","title":"code_parser","text":"<p>langroid/parsing/code_parser.py </p>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser","title":"<code>CodeParser(config)</code>","text":"Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def __init__(self, config: CodeParsingConfig):\nself.config = config\nself.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.num_tokens","title":"<code>num_tokens(text)</code>","text":"<p>How many tokens are in the text, according to the tokenizer. This needs to be accurate, otherwise we may exceed the maximum number of tokens allowed by the model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string to tokenize</p> required <p>Returns:</p> Type Description <code>int</code> <p>number of tokens in the text</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def num_tokens(self, text: str) -&gt; int:\n\"\"\"\n    How many tokens are in the text, according to the tokenizer.\n    This needs to be accurate, otherwise we may exceed the maximum\n    number of tokens allowed by the model.\n    Args:\n        text: string to tokenize\n    Returns:\n        number of tokens in the text\n    \"\"\"\ntokens = self.tokenizer.encode(text)\nreturn len(tokens)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.split","title":"<code>split(docs)</code>","text":"<p>Split the documents into chunks, according to the config.splitter. Only the documents with a language in the config.extensions are split.</p> <p>Note</p> <p>We assume the metadata in each document has at least a <code>language</code> field, which is used to determine how to chunk the code.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of documents to split</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of documents, where each document is a chunk; the metadata of the</p> <code>List[Document]</code> <p>original document is duplicated for each chunk, so that when we retrieve a</p> <code>List[Document]</code> <p>chunk, we immediately know info about the original document.</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def split(self, docs: List[Document]) -&gt; List[Document]:\n\"\"\"\n    Split the documents into chunks, according to the config.splitter.\n    Only the documents with a language in the config.extensions are split.\n    !!! note\n        We assume the metadata in each document has at least a `language` field,\n        which is used to determine how to chunk the code.\n    Args:\n        docs: list of documents to split\n    Returns:\n        list of documents, where each document is a chunk; the metadata of the\n        original document is duplicated for each chunk, so that when we retrieve a\n        chunk, we immediately know info about the original document.\n    \"\"\"\nchunked_docs = [\n[\nDocument(content=chunk, metadata=d.metadata)\nfor chunk in chunk_code(\nd.content,\nd.metadata.language,  # type: ignore\nself.config.chunk_size,\nself.num_tokens,\n)\nif chunk.strip() != \"\"\n]\nfor d in docs\nif d.metadata.language in self.config.extensions  # type: ignore\n]\n# collapse the list of lists into a single list\nreturn reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.chunk_code","title":"<code>chunk_code(code, language, max_tokens, len_fn)</code>","text":"<p>Chunk code into smaller pieces, so that we don't exceed the maximum number of tokens allowed by the embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>string of code</p> required <code>language</code> <code>str</code> <p>str as a file extension, e.g. \"py\", \"yml\"</p> required <code>max_tokens</code> <code>int</code> <p>max tokens per chunk</p> required <code>len_fn</code> <code>Callable[[str], int]</code> <p>function to get the length of a string in token units</p> required Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def chunk_code(\ncode: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\n\"\"\"\n    Chunk code into smaller pieces, so that we don't exceed the maximum\n    number of tokens allowed by the embedding model.\n    Args:\n        code: string of code\n        language: str as a file extension, e.g. \"py\", \"yml\"\n        max_tokens: max tokens per chunk\n        len_fn: function to get the length of a string in token units\n    Returns:\n    \"\"\"\nlexer = get_lexer_by_name(language)\ntokens = list(lex(code, lexer))\nchunks = []\ncurrent_chunk = \"\"\nfor token_type, token_value in tokens:\nif token_type in Token.Text.Whitespace:\ncurrent_chunk += token_value\nelse:\ntoken_tokens = len_fn(token_value)\nif len_fn(current_chunk) + token_tokens &lt;= max_tokens:\ncurrent_chunk += token_value\nelse:\nchunks.append(current_chunk)\ncurrent_chunk = token_value\nif current_chunk:\nchunks.append(current_chunk)\nreturn chunks\n</code></pre>"},{"location":"reference/parsing/json/","title":"json","text":"<p>langroid/parsing/json.py </p>"},{"location":"reference/parsing/json/#langroid.parsing.json.is_valid_json","title":"<code>is_valid_json(json_str)</code>","text":"<p>Check if the input string is a valid JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The input string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string is a valid JSON, False otherwise.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def is_valid_json(json_str: str) -&gt; bool:\n\"\"\"Check if the input string is a valid JSON.\n    Args:\n        json_str (str): The input string to check.\n    Returns:\n        bool: True if the input string is a valid JSON, False otherwise.\n    \"\"\"\ntry:\njson.loads(json_str)\nreturn True\nexcept ValueError:\nreturn False\n</code></pre>"},{"location":"reference/parsing/json/#langroid.parsing.json.extract_top_level_json","title":"<code>extract_top_level_json(s)</code>","text":"<p>Extract all top-level JSON-formatted substrings from a given string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of top-level JSON-formatted substrings.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def extract_top_level_json(s: str) -&gt; List[str]:\n\"\"\"Extract all top-level JSON-formatted substrings from a given string.\n    Args:\n        s (str): The input string to search for JSON substrings.\n    Returns:\n        List[str]: A list of top-level JSON-formatted substrings.\n    \"\"\"\n# Find JSON object and array candidates using regular expressions\njson_candidates = regex.findall(r\"(?&lt;!\\\\)(?:\\\\\\\\)*\\{(?:[^{}]|(?R))*\\}\", s)\ntop_level_jsons = [\ncandidate for candidate in json_candidates if is_valid_json(candidate)\n]\nreturn top_level_jsons\n</code></pre>"},{"location":"reference/parsing/para_sentence_split/","title":"para_sentence_split","text":"<p>langroid/parsing/para_sentence_split.py </p>"},{"location":"reference/parsing/parser/","title":"parser","text":"<p>langroid/parsing/parser.py </p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser","title":"<code>Parser(config)</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>def __init__(self, config: ParsingConfig):\nself.config = config\nself.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\nself,\ntext: str,\n) -&gt; List[str]:\n\"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n    Args:\n        text: The text to split into chunks.\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n# Return an empty list if the text is empty or whitespace\nif not text or text.isspace():\nreturn []\n# Tokenize the text\ntokens = self.tokenizer.encode(text, disallowed_special=())\n# Initialize an empty list of chunks\nchunks = []\n# Initialize a counter for the number of chunks\nnum_chunks = 0\n# Loop until all tokens are consumed\nwhile tokens and num_chunks &lt; self.config.max_chunks:\n# Take the first chunk_size tokens as a chunk\nchunk = tokens[: self.config.chunk_size]\n# Decode the chunk into text\nchunk_text = self.tokenizer.decode(chunk)\n# Skip the chunk if it is empty or whitespace\nif not chunk_text or chunk_text.isspace():\n# Remove the tokens corresponding to the chunk text\n# from remaining tokens\ntokens = tokens[len(chunk) :]\n# Continue to the next iteration of the loop\ncontinue\n# Find the last period or punctuation mark in the chunk\nlast_punctuation = max(\nchunk_text.rfind(\".\"),\nchunk_text.rfind(\"?\"),\nchunk_text.rfind(\"!\"),\nchunk_text.rfind(\"\\n\"),\n)\n# If there is a punctuation mark, and the last punctuation index is\n# after MIN_CHUNK_SIZE_CHARS\nif (\nlast_punctuation != -1\nand last_punctuation &gt; self.config.min_chunk_chars\n):\n# Truncate the chunk text at the punctuation mark\nchunk_text = chunk_text[: last_punctuation + 1]\n# Remove any newline characters and strip any leading or\n# trailing whitespace\nchunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\nif len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n# Append the chunk text to the list of chunks\nchunks.append(chunk_text_to_append)\n# Remove the tokens corresponding to the chunk text\n# from the remaining tokens\ntokens = tokens[\nlen(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n]\n# Increment the number of chunks\nnum_chunks += 1\n# Handle the remaining tokens\nif tokens:\nremaining_text = self.tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\nif len(remaining_text) &gt; self.config.discard_chunk_chars:\nchunks.append(remaining_text)\nreturn chunks\n</code></pre>"},{"location":"reference/parsing/pdf_parser/","title":"pdf_parser","text":"<p>langroid/parsing/pdf_parser.py </p>"},{"location":"reference/parsing/pdf_parser/#langroid.parsing.pdf_parser.get_doc_from_pdf_url","title":"<code>get_doc_from_pdf_url(url)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>contains the URL to the PDF file</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the content of the pdf file, and metadata containing url</p> Source code in <code>langroid/parsing/pdf_parser.py</code> <pre><code>def get_doc_from_pdf_url(url: str) -&gt; Document:\n\"\"\"\n    Args:\n        url (str): contains the URL to the PDF file\n    Returns:\n        a `Document` object containing the content of the pdf file,\n            and metadata containing url\n    \"\"\"\nresponse = requests.get(url)\nresponse.raise_for_status()\nwith BytesIO(response.content) as f:\nreader = PdfReader(f)\ntext = _text_from_pdf_reader(reader)\nreturn Document(content=text, metadata=DocMetaData(source=str(url)))\n</code></pre>"},{"location":"reference/parsing/pdf_parser/#langroid.parsing.pdf_parser.get_doc_from_pdf_file","title":"<code>get_doc_from_pdf_file(path)</code>","text":"<p>Given local path to a PDF file, extract the text content.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>full path to the PDF file PDF file obtained via URL</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the content of the pdf file, and metadata containing path/url</p> Source code in <code>langroid/parsing/pdf_parser.py</code> <pre><code>def get_doc_from_pdf_file(path: str) -&gt; Document:\n\"\"\"\n    Given local path to a PDF file, extract the text content.\n    Args:\n        path (str): full path to the PDF file\n            PDF file obtained via URL\n    Returns:\n        a `Document` object containing the content of the pdf file,\n            and metadata containing path/url\n    \"\"\"\nreader = PdfReader(path)\ntext = _text_from_pdf_reader(reader)\nreturn Document(content=text, metadata=DocMetaData(source=str(path)))\n</code></pre>"},{"location":"reference/parsing/repo_loader/","title":"repo_loader","text":"<p>langroid/parsing/repo_loader.py </p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoaderConfig","title":"<code>RepoLoaderConfig</code>","text":"<p>             Bases: <code>BaseSettings</code></p> <p>Configuration for RepoLoader.</p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader","title":"<code>RepoLoader(url, config=RepoLoaderConfig())</code>","text":"<p>Class for recursively getting all file content in a repo.</p> <pre><code>config: configuration for RepoLoader\n</code></pre> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def __init__(\nself,\nurl: str,\nconfig: RepoLoaderConfig = RepoLoaderConfig(),\n):\n\"\"\"\n    Args:\n        url: full github url of repo, or just \"owner/repo\"\n        config: configuration for RepoLoader\n    \"\"\"\nself.url = url\nself.config = config\nself.clone_path: Optional[str] = None\nself.log_file = \".logs/repo_loader/download_log.json\"\nos.makedirs(os.path.dirname(self.log_file), exist_ok=True)\nif not os.path.exists(self.log_file):\nwith open(self.log_file, \"w\") as f:\njson.dump({\"junk\": \"ignore\"}, f)\nwith open(self.log_file, \"r\") as f:\nlog = json.load(f)\nif self.url in log:\nlogger.info(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nif \"github.com\" in self.url:\nrepo_name = self.url.split(\"github.com/\")[1]\nelse:\nrepo_name = self.url\nload_dotenv()\n# authenticated calls to github api have higher rate limit\ntoken = os.getenv(\"GITHUB_ACCESS_TOKEN\")\ng = Github(token)\nself.repo = self._get_repo_with_retry(g, repo_name)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.clone","title":"<code>clone(path=None)</code>","text":"<p>Clone a GitHub repository to a local directory specified by <code>path</code>, if it has not already been cloned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local directory where the repository should be cloned. If not specified, a temporary directory will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The path to the local directory where the repository was cloned.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n\"\"\"\n    Clone a GitHub repository to a local directory specified by `path`,\n    if it has not already been cloned.\n    Args:\n        path (str): The local directory where the repository should be cloned.\n            If not specified, a temporary directory will be created.\n    Returns:\n        str: The path to the local directory where the repository was cloned.\n    \"\"\"\nwith open(self.log_file, \"r\") as f:\nlog: Dict[str, str] = json.load(f)\nif self.url in log and os.path.exists(log[self.url]):\nlogger.warning(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nreturn self.clone_path\nself.clone_path = path\nif path is None:\npath = self.default_clone_path()\nself.clone_path = path\ntry:\nsubprocess.run([\"git\", \"clone\", self.url, path], check=True)\nlog[self.url] = path\nwith open(self.log_file, \"w\") as f:\njson.dump(log, f)\nreturn self.clone_path\nexcept subprocess.CalledProcessError as e:\nlogger.error(f\"Git clone failed: {e}\")\nexcept Exception as e:\nlogger.error(f\"An error occurred while trying to clone the repository:{e}\")\nreturn self.clone_path\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_tree_from_github","title":"<code>load_tree_from_github(depth, lines=0)</code>","text":"<p>Get a nested dictionary of GitHub repository file and directory names up to a certain depth, with file contents.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth level.</p> required <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]:</p> <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>A dictionary containing file and directory names, with file contents.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_tree_from_github(\nself, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n    Get a nested dictionary of GitHub repository file and directory names\n    up to a certain depth, with file contents.\n    Args:\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n    Returns:\n        Dict[str, Union[str, List[Dict]]]:\n        A dictionary containing file and directory names, with file contents.\n    \"\"\"\nroot_contents = self.repo.get_contents(\"\")\nif not isinstance(root_contents, list):\nroot_contents = [root_contents]\nrepo_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_node, current_depth, parent_structure)\nqueue = deque([(root_contents, 0, repo_structure)])\nwhile queue:\ncurrent_node, current_depth, parent_structure = queue.popleft()\nfor content in current_node:\nif not self._is_allowed(content):\ncontinue\nif content.type == \"dir\" and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": content.name,\n\"dirs\": [],\n\"files\": [],\n\"path\": content.path,\n}\nparent_structure[\"dirs\"].append(new_dir)\ncontents = self.repo.get_contents(content.path)\nif not isinstance(contents, list):\ncontents = [contents]\nqueue.append(\n(\ncontents,\ncurrent_depth + 1,\nnew_dir,\n)\n)\nelif content.type == \"file\":\nfile_content = \"\\n\".join(\n_get_decoded_content(content).splitlines()[:lines]\n)\nfile_dict = {\n\"type\": \"file\",\n\"name\": content.name,\n\"content\": file_content,\n\"path\": content.path,\n}\nparent_structure[\"files\"].append(file_dict)\nreturn repo_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load","title":"<code>load(path=None, depth=3, lines=0)</code>","text":"<p>From a local folder <code>path</code> (if None, the repo clone path), get:   a nested dictionary (tree) of dicts, files and contents   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path; if none, use self.clone_path()</p> <code>None</code> <code>depth</code> <code>int</code> <p>The depth level.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents, and A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load(\nself,\npath: Optional[str] = None,\ndepth: int = 3,\nlines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n    From a local folder `path` (if None, the repo clone path), get:\n      a nested dictionary (tree) of dicts, files and contents\n      a list of Document objects for each file.\n    Args:\n        path (str): The local folder path; if none, use self.clone_path()\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n    Returns:\n        Tuple of (dict, List_of_Documents):\n          A dictionary containing file and directory names, with file contents, and\n          A list of Document objects for each file.\n    \"\"\"\nif path is None:\nif self.clone_path is None:\nself.clone()\npath = self.clone_path\nif path is None:\nraise ValueError(\"Unable to clone repo\")\nreturn self.load_from_folder(\npath=path,\ndepth=depth,\nlines=lines,\nfile_types=self.config.file_types,\nexclude_dirs=self.config.exclude_dirs,\nurl=self.url,\n)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_from_folder","title":"<code>load_from_folder(path, depth=3, lines=0, file_types=None, exclude_dirs=None, url='')</code>  <code>staticmethod</code>","text":"<p>From a local folder <code>path</code> (required), get:   a nested dictionary (tree) of dicts, files and contents, restricting to     desired file_types and excluding undesired directories.   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path, required.</p> required <code>depth</code> <code>int</code> <p>The depth level. Optional, default 3.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.     Optional, default 0 (no lines =&gt; empty string).</p> <code>0</code> <code>file_types</code> <code>List[str]</code> <p>The file types to include.     Optional, default None (all).</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>The directories to exclude.     Optional, default None (no exclusions).</p> <code>None</code> <code>url</code> <code>str</code> <p>Optional url, to be stored in docs as metadata. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents. A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef load_from_folder(\npath: str,\ndepth: int = 3,\nlines: int = 0,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\nurl: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n    From a local folder `path` (required), get:\n      a nested dictionary (tree) of dicts, files and contents, restricting to\n        desired file_types and excluding undesired directories.\n      a list of Document objects for each file.\n    Args:\n        path (str): The local folder path, required.\n        depth (int): The depth level. Optional, default 3.\n        lines (int): The number of lines of file contents to include.\n                Optional, default 0 (no lines =&gt; empty string).\n        file_types (List[str]): The file types to include.\n                Optional, default None (all).\n        exclude_dirs (List[str]): The directories to exclude.\n                Optional, default None (no exclusions).\n        url (str): Optional url, to be stored in docs as metadata. Default \"\".\n    Returns:\n        Tuple of (dict, List_of_Documents):\n          A dictionary containing file and directory names, with file contents.\n          A list of Document objects for each file.\n    \"\"\"\nfolder_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_path, current_depth, parent_structure)\nqueue = deque([(path, 0, folder_structure)])\ndocs = []\nexclude_dirs = exclude_dirs or []\nwhile queue:\ncurrent_path, current_depth, parent_structure = queue.popleft()\nfor item in os.listdir(current_path):\nitem_path = os.path.join(current_path, item)\nrelative_path = os.path.relpath(item_path, path)\nif (os.path.isdir(item_path) and item in exclude_dirs) or (\nos.path.isfile(item_path)\nand file_types is not None\nand RepoLoader._file_type(item) not in file_types\n):\ncontinue\nif os.path.isdir(item_path) and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": item,\n\"dirs\": [],\n\"files\": [],\n\"path\": relative_path,\n}\nparent_structure[\"dirs\"].append(new_dir)\nqueue.append((item_path, current_depth + 1, new_dir))\nelif os.path.isfile(item_path):\n# Add the file to the current dictionary\nwith open(item_path, \"r\") as f:\nfile_lines = list(itertools.islice(f, lines))\nfile_content = \"\\n\".join(line.strip() for line in file_lines)\nif file_content == \"\":\ncontinue\nfile_dict = {\n\"type\": \"file\",\n\"name\": item,\n\"content\": file_content,\n\"path\": relative_path,\n}\nparent_structure[\"files\"].append(file_dict)\ndocs.append(\nDocument(\ncontent=file_content,\nmetadata=DocMetaData(\nrepo=url,\nsource=relative_path,\nurl=url,\nfilename=item,\nextension=RepoLoader._file_type(item),\nlanguage=RepoLoader._file_type(item),\n),\n)\n)\nreturn folder_structure, docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_documents","title":"<code>get_documents(path, file_types=None, exclude_dirs=None, depth=-1, lines=None)</code>  <code>staticmethod</code>","text":"<p>Recursively get all files under a path as Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory or file.</p> required <code>file_types</code> <code>List[str]</code> <p>List of file extensions OR filenames OR file_path_names to  include. Defaults to None, which includes all files.</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>List of directories to exclude. Defaults to None, which includes all directories.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Max depth of recursion. Defaults to -1, which includes all depths.</p> <code>-1</code> <code>lines</code> <code>int</code> <p>Number of lines to read from each file. Defaults to None, which reads all lines.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Document objects representing files.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef get_documents(\npath: str,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\ndepth: int = -1,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n    Recursively get all files under a path as Document objects.\n    Args:\n        path (str): The path to the directory or file.\n        file_types (List[str], optional): List of file extensions OR\n            filenames OR file_path_names to  include.\n            Defaults to None, which includes all files.\n        exclude_dirs (List[str], optional): List of directories to exclude.\n            Defaults to None, which includes all directories.\n        depth (int, optional): Max depth of recursion. Defaults to -1,\n            which includes all depths.\n        lines (int, optional): Number of lines to read from each file.\n            Defaults to None, which reads all lines.\n    Returns:\n        List[Document]: List of Document objects representing files.\n    \"\"\"\ndocs = []\nfile_paths = []\npath_obj = Path(path).resolve()\nif path_obj.is_file():\nfile_paths.append(str(path_obj))\nelse:\npath_depth = len(path_obj.parts)\nfor root, dirs, files in os.walk(path):\n# Exclude directories if needed\nif exclude_dirs:\ndirs[:] = [d for d in dirs if d not in exclude_dirs]\ncurrent_depth = len(Path(root).resolve().parts) - path_depth\nif depth == -1 or current_depth &lt;= depth:\nfor file in files:\nfile_path = str(Path(root) / file)\nif (\nfile_types is None\nor RepoLoader._file_type(file_path) in file_types\nor os.path.basename(file_path) in file_types\nor file_path in file_types\n):\nfile_paths.append(file_path)\nfor file_path in file_paths:\n_, file_extension = os.path.splitext(file_path)\nif file_extension == \".pdf\":\ndocs.append(get_doc_from_pdf_file(file_path))\nelse:\nwith open(file_path, \"r\") as f:\nif lines is not None:\nfile_lines = list(itertools.islice(f, lines))\ncontent = \"\\n\".join(line.strip() for line in file_lines)\nelse:\ncontent = f.read()\nsoup = BeautifulSoup(content, \"html.parser\")\ntext = soup.get_text()\ndocs.append(\nDocument(\ncontent=text,\nmetadata=DocMetaData(source=str(file_path)),\n)\n)\nreturn docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_docs_from_github","title":"<code>load_docs_from_github(k=None, depth=None, lines=None)</code>","text":"<p>Directly from GitHub, recursively get all files in a repo that have one of the extensions, possibly up to a max number of files, max depth, and max number of lines per file (if any of these are specified).</p> <p>Parameters:</p> Name Type Description Default <code>k(int)</code> <p>max number of files to load, or None for all files</p> required <code>depth(int)</code> <p>max depth to recurse, or None for infinite depth</p> required <code>lines</code> <code>int</code> <p>max number of lines to get, from a file, or None for all lines</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects, each has fields <code>content</code> and <code>metadata</code>,</p> <code>List[Document]</code> <p>and <code>metadata</code> has fields <code>url</code>, <code>filename</code>, <code>extension</code>, <code>language</code></p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_docs_from_github(\nself,\nk: Optional[int] = None,\ndepth: Optional[int] = None,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n    Directly from GitHub, recursively get all files in a repo that have one of the\n    extensions, possibly up to a max number of files, max depth, and max number\n    of lines per file (if any of these are specified).\n    Args:\n        k(int): max number of files to load, or None for all files\n        depth(int): max depth to recurse, or None for infinite depth\n        lines (int): max number of lines to get, from a file, or None for all lines\n    Returns:\n        list of Document objects, each has fields `content` and `metadata`,\n        and `metadata` has fields `url`, `filename`, `extension`, `language`\n    \"\"\"\ncontents = self.repo.get_contents(\"\")\nif not isinstance(contents, list):\ncontents = [contents]\nstack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n# recursively get all files in repo that have one of the extensions\ndocs = []\ni = 0\nwhile stack:\nif k is not None and i == k:\nbreak\nfile_content, d = stack.pop()\nif not self._is_allowed(file_content):\ncontinue\nif file_content.type == \"dir\":\nif depth is None or d &lt;= depth:\nitems = self.repo.get_contents(file_content.path)\nif not isinstance(items, list):\nitems = [items]\nstack.extend(list(zip(items, [d + 1] * len(items))))\nelse:\nif depth is None or d &lt;= depth:\n# need to decode the file content, which is in bytes\ncontents = self.repo.get_contents(file_content.path)\nif isinstance(contents, list):\ncontents = contents[0]\ntext = _get_decoded_content(contents)\nif lines is not None:\ntext = \"\\n\".join(text.split(\"\\n\")[:lines])\ni += 1\n# Note `source` is important, it may be used to cite\n# evidence for an answer.\n# See  URLLoader\n# TODO we should use Pydantic to enforce/standardize this\ndocs.append(\nDocument(\ncontent=text,\nmetadata=DocMetaData(\nrepo=self.url,\nsource=file_content.html_url,\nurl=file_content.html_url,\nfilename=file_content.name,\nextension=self._file_type(file_content.name),\nlanguage=self._file_type(file_content.name),\n),\n)\n)\nreturn docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.select","title":"<code>select(structure, includes, excludes=[])</code>  <code>staticmethod</code>","text":"<p>Filter a structure dictionary for certain directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>includes</code> <code>List[str]</code> <p>A list of desired directories and files. For files, either full file names or \"file type\" can be specified. E.g.  \"toml\" will include all files with the \".toml\" extension, or \"Makefile\" will include all files named \"Makefile\".</p> required <code>excludes</code> <code>List[str]</code> <p>A list of directories and files to exclude. Similar to <code>includes</code>, full file/dir names or \"file type\" can be specified. Optional, defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef select(\nstructure: Dict[str, Union[str, List[Dict[str, Any]]]],\nincludes: List[str],\nexcludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n    Filter a structure dictionary for certain directories and files.\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        includes (List[str]): A list of desired directories and files.\n            For files, either full file names or \"file type\" can be specified.\n            E.g.  \"toml\" will include all files with the \".toml\" extension,\n            or \"Makefile\" will include all files named \"Makefile\".\n        excludes (List[str]): A list of directories and files to exclude.\n            Similar to `includes`, full file/dir names or \"file type\" can be\n            specified. Optional, defaults to empty list.\n    Returns:\n        Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n    \"\"\"\nfiltered_structure = {\n\"type\": structure[\"type\"],\n\"name\": structure[\"name\"],\n\"dirs\": [],\n\"files\": [],\n\"path\": structure[\"path\"],\n}\nfor dir in structure[\"dirs\"]:\nif (\ndir[\"name\"] in includes\nor RepoLoader._file_type(dir[\"name\"]) in includes\n) and (\ndir[\"name\"] not in excludes\nand RepoLoader._file_type(dir[\"name\"]) not in excludes\n):\n# If the directory is in the select list, include the whole subtree\nfiltered_structure[\"dirs\"].append(dir)\nelse:\n# Otherwise, filter the directory's contents\nfiltered_dir = RepoLoader.select(dir, includes)\nif (\nfiltered_dir[\"dirs\"] or filtered_dir[\"files\"]\n):  # only add if not empty\nfiltered_structure[\"dirs\"].append(filtered_dir)\nfor file in structure[\"files\"]:\nif (\nfile[\"name\"] in includes\nor RepoLoader._file_type(file[\"name\"]) in includes\n) and (\nfile[\"name\"] not in excludes\nand RepoLoader._file_type(file[\"name\"]) not in excludes\n):\nfiltered_structure[\"files\"].append(file)\nreturn filtered_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.ls","title":"<code>ls(structure, depth=0)</code>  <code>staticmethod</code>","text":"<p>Get a list of names of files or directories up to a certain depth from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of names of files or directories.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n\"\"\"\n    Get a list of names of files or directories up to a certain depth from a\n    structure dictionary.\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        depth (int, optional): The depth level. Defaults to 0.\n    Returns:\n        List[str]: A list of names of files or directories.\n    \"\"\"\nnames = []\n# A queue of tuples (current_structure, current_depth)\nqueue = deque([(structure, 0)])\nwhile queue:\ncurrent_structure, current_depth = queue.popleft()\nif current_depth &lt;= depth:\nnames.append(current_structure[\"name\"])\nfor dir in current_structure[\"dirs\"]:\nqueue.append((dir, current_depth + 1))\nfor file in current_structure[\"files\"]:\n# add file names only if depth is less than the limit\nif current_depth &lt; depth:\nnames.append(file[\"name\"])\nnames = [n for n in names if n not in [\"\", None]]\nreturn names\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.list_files","title":"<code>list_files(dir, depth=1, include_types=[], exclude_types=[])</code>  <code>staticmethod</code>","text":"<p>Recursively list all files in a directory, up to a certain depth.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory path, relative to root.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 1.</p> <code>1</code> <code>include_types</code> <code>List[str]</code> <p>A list of file types to include. Defaults to empty list.</p> <code>[]</code> <code>exclude_types</code> <code>List[str]</code> <p>A list of file types to exclude. Defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file names.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef list_files(\ndir: str,\ndepth: int = 1,\ninclude_types: List[str] = [],\nexclude_types: List[str] = [],\n) -&gt; List[str]:\n\"\"\"\n    Recursively list all files in a directory, up to a certain depth.\n    Args:\n        dir (str): The directory path, relative to root.\n        depth (int, optional): The depth level. Defaults to 1.\n        include_types (List[str], optional): A list of file types to include.\n            Defaults to empty list.\n        exclude_types (List[str], optional): A list of file types to exclude.\n            Defaults to empty list.\n    Returns:\n        List[str]: A list of file names.\n    \"\"\"\ndepth = depth if depth &gt;= 0 else 200\noutput = []\nfor root, dirs, files in os.walk(dir):\nif root.count(os.sep) - dir.count(os.sep) &lt; depth:\nlevel = root.count(os.sep) - dir.count(os.sep)\nsub_indent = \" \" * 4 * (level + 1)\nfor d in dirs:\noutput.append(\"{}{}/\".format(sub_indent, d))\nfor f in files:\nif include_types and RepoLoader._file_type(f) not in include_types:\ncontinue\nif exclude_types and RepoLoader._file_type(f) in exclude_types:\ncontinue\noutput.append(\"{}{}\".format(sub_indent, f))\nreturn output\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.show_file_contents","title":"<code>show_file_contents(tree)</code>  <code>staticmethod</code>","text":"<p>Print the contents of all files from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n\"\"\"\n    Print the contents of all files from a structure dictionary.\n    Args:\n        tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n    \"\"\"\ncontents = \"\"\nfor dir in tree[\"dirs\"]:\ncontents += RepoLoader.show_file_contents(dir)\nfor file in tree[\"files\"]:\npath = file[\"path\"]\ncontents += f\"\"\"\n{path}:\n        --------------------\n{file[\"content\"]}\n        \"\"\"\nreturn contents\n</code></pre>"},{"location":"reference/parsing/url_loader/","title":"url_loader","text":"<p>langroid/parsing/url_loader.py </p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader","title":"<code>URLLoader(urls)</code>","text":"<p>Load a list of URLs and extract the text content. Alternative approaches could use <code>bs4</code> or <code>scrapy</code>.</p> <p>TODO - this currently does not handle cookie dialogs,  i.e. if there is a cookie pop-up, most/all of the extracted  content could be cookie policy text.  We could use <code>playwright</code> to simulate a user clicking  the \"accept\" button on the cookie dialog.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>def __init__(self, urls: List[str]):\nself.urls = urls\n</code></pre>"},{"location":"reference/parsing/urls/","title":"urls","text":"<p>langroid/parsing/urls.py </p>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_user_input","title":"<code>get_user_input(msg, color='blue')</code>","text":"<p>Prompt the user for input.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>printed prompt</p> required <code>color</code> <code>str</code> <p>color of the prompt</p> <code>'blue'</code> <p>Returns:</p> Type Description <code>str</code> <p>user input</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_user_input(msg: str, color: str = \"blue\") -&gt; str:\n\"\"\"\n    Prompt the user for input.\n    Args:\n        msg: printed prompt\n        color: color of the prompt\n    Returns:\n        user input\n    \"\"\"\ncolor_str = f\"[{color}]{msg} \" if color else msg + \" \"\nprint(color_str, end=\"\")\nreturn input(\"\")\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_list_from_user","title":"<code>get_list_from_user(prompt=\"Enter input (type 'done' or hit return to finish)\", n=None)</code>","text":"<p>Prompt the user for inputs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>printed prompt</p> <code>\"Enter input (type 'done' or hit return to finish)\"</code> <code>n</code> <code>Optional[int]</code> <p>how many inputs to prompt for. If None, then prompt until done, otherwise quit after n inputs.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of input strings</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_list_from_user(\nprompt: str = \"Enter input (type 'done' or hit return to finish)\",\nn: Optional[int] = None,\n) -&gt; List[str]:\n\"\"\"\n    Prompt the user for inputs.\n    Args:\n        prompt: printed prompt\n        n: how many inputs to prompt for. If None, then prompt until done, otherwise\n            quit after n inputs.\n    Returns:\n        list of input strings\n    \"\"\"\n# Create an empty set to store the URLs.\ninput_set = set()\n# Use a while loop to continuously ask the user for URLs.\ni = 0\nwhile True:\n# Prompt the user for input.\ninput_str = Prompt.ask(f\"[blue]{prompt}\")\n# Check if the user wants to exit the loop.\nif input_str.lower() == \"done\" or input_str == \"\":\nbreak\ninput_set.add(input_str.strip())\ni += 1\nif i == n:\nbreak\nreturn list(input_set)\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_urls_and_paths","title":"<code>get_urls_and_paths(inputs)</code>","text":"<p>Given a list of inputs, return a list of URLs and a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>list of strings</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>list of URLs, list of paths</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_urls_and_paths(inputs: List[str]) -&gt; Tuple[List[str], List[str]]:\n\"\"\"\n    Given a list of inputs, return a list of URLs and a list of paths.\n    Args:\n        inputs: list of strings\n    Returns:\n        list of URLs, list of paths\n    \"\"\"\nurls = []\npaths = []\nfor item in inputs:\ntry:\nm = Url(url=parse_obj_as(HttpUrl, item))\nurls.append(str(m.url))\nexcept ValidationError:\nif os.path.exists(item):\npaths.append(item)\nelse:\nlogger.warning(f\"{item} is neither a URL nor a path.\")\nreturn urls, paths\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.find_urls","title":"<code>find_urls(url='https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', visited=None, depth=0, max_depth=2)</code>","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>visited</code> <code>Optional[Set[str]]</code> <code>None</code> <code>depth</code> <code>int</code> <code>0</code> <code>max_depth</code> <code>int</code> <code>2</code> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\nurl: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\nvisited: Optional[Set[str]] = None,\ndepth: int = 0,\nmax_depth: int = 2,\n) -&gt; Set[str]:\n\"\"\"\n    Recursively find all URLs on a given page.\n    Args:\n        url:\n        visited:\n        depth:\n        max_depth:\n    Returns:\n    \"\"\"\nif visited is None:\nvisited = set()\nvisited.add(url)\ntry:\nresponse = requests.get(url)\nresponse.raise_for_status()\nexcept (\nrequests.exceptions.HTTPError,\nrequests.exceptions.RequestException,\n):\nprint(f\"Failed to fetch '{url}'\")\nreturn visited\nsoup = BeautifulSoup(response.content, \"html.parser\")\nlinks = soup.find_all(\"a\", href=True)\nurls = [urljoin(url, link[\"href\"]) for link in links]  # Construct full URLs\nif depth &lt; max_depth:\nfor link_url in urls:\nif link_url not in visited:\nfind_urls(link_url, visited, depth + 1, max_depth)\nreturn visited\n</code></pre>"},{"location":"reference/parsing/utils/","title":"utils","text":"<p>langroid/parsing/utils.py </p>"},{"location":"reference/prompts/","title":"prompts","text":"<p>langroid/prompts/init.py </p>"},{"location":"reference/prompts/dialog/","title":"dialog","text":"<p>langroid/prompts/dialog.py </p>"},{"location":"reference/prompts/dialog/#langroid.prompts.dialog.collate_chat_history","title":"<code>collate_chat_history(inputs)</code>","text":"<p>Collate (human, ai) pairs into a single, string</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[tuple[str, str]]</code> required Source code in <code>langroid/prompts/dialog.py</code> <pre><code>def collate_chat_history(inputs: List[tuple[str, str]]) -&gt; str:\n\"\"\"\n    Collate (human, ai) pairs into a single, string\n    Args:\n        inputs:\n    Returns:\n    \"\"\"\npairs = [\nf\"\"\"Human:{human}\n        AI:{ai}\n        \"\"\"\nfor human, ai in inputs\n]\nreturn \"\\n\".join(pairs)\n</code></pre>"},{"location":"reference/prompts/prompts_config/","title":"prompts_config","text":"<p>langroid/prompts/prompts_config.py </p>"},{"location":"reference/prompts/templates/","title":"templates","text":"<p>langroid/prompts/templates.py </p>"},{"location":"reference/prompts/transforms/","title":"transforms","text":"<p>langroid/prompts/transforms.py </p>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage, LLM)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question.</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>async def get_verbatim_extract_async(\nquestion: str,\npassage: Document,\nLLM: LanguageModel,\n) -&gt; str:\n\"\"\"\n    Asynchronously, get verbatim extract from passage that is relevant to a question.\n    \"\"\"\nasync with aiohttp.ClientSession():\ntemplatized_prompt = EXTRACTION_PROMPT\nfinal_prompt = templatized_prompt.format(question=question, content=passage)\nfinal_extract = await LLM.agenerate(prompt=final_prompt, max_tokens=1024)\nreturn final_extract.message.strip()\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages, LLM)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to be answered</p> required <code>passages</code> <code>List[Document]</code> <p>list of passages from which to extract relevant verbatim text</p> required <code>LLM</code> <code>LanguageModel</code> <p>LanguageModel to use for generating the prompt and extract</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of verbatim extracts (Documents) from passages that are relevant to</p> <code>List[Document]</code> <p>question</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def get_verbatim_extracts(\nquestion: str,\npassages: List[Document],\nLLM: LanguageModel,\n) -&gt; List[Document]:\n\"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts (Documents) from passages that are relevant to\n        question\n    \"\"\"\nreturn asyncio.run(_get_verbatim_extracts(question, passages, LLM))\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.followup_to_standalone","title":"<code>followup_to_standalone(LLM, chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>List[Tuple[str, str]]</code> <p>list of tuples of (question, answer)</p> required <code>query</code> <p>follow-up question</p> required Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def followup_to_standalone(\nLLM: LanguageModel, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n\"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n    Returns: standalone version of the question\n    \"\"\"\nhistory = collate_chat_history(chat_history)\nprompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n    Chat history: {history}\n    Follow-up question: {question}     \"\"\".strip()\nstandalone = LLM.generate(prompt=prompt, max_tokens=1024).message.strip()\nreturn standalone\n</code></pre>"},{"location":"reference/scripts/","title":"scripts","text":"<p>langroid/scripts/init.py </p>"},{"location":"reference/utils/","title":"utils","text":"<p>langroid/utils/init.py </p>"},{"location":"reference/utils/configuration/","title":"configuration","text":"<p>langroid/utils/configuration.py </p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.update_global_settings","title":"<code>update_global_settings(cfg, keys)</code>","text":"<p>Update global settings so modules can access them via (as an example): <pre><code>from langroid.utils.configuration import settings\nif settings.debug...\n</code></pre> Caution we do not want to have too many such global settings!</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>BaseSettings</code> <p>pydantic config, typically from a main script</p> required <code>keys</code> <code>List[str]</code> <p>which keys from cfg to use, to update the global settings object</p> required Source code in <code>langroid/utils/configuration.py</code> <pre><code>def update_global_settings(cfg: BaseSettings, keys: List[str]) -&gt; None:\n\"\"\"\n    Update global settings so modules can access them via (as an example):\n    ```\n    from langroid.utils.configuration import settings\n    if settings.debug...\n    ```\n    Caution we do not want to have too many such global settings!\n    Args:\n        cfg: pydantic config, typically from a main script\n        keys: which keys from cfg to use, to update the global settings object\n    \"\"\"\nconfig_dict = cfg.dict()\n# Filter the config_dict based on the keys\nfiltered_config = {key: config_dict[key] for key in keys if key in config_dict}\n# create a new Settings() object to let pydantic validate it\nnew_settings = Settings(**filtered_config)\n# Update the unique global settings object\nsettings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":"<p>langroid/utils/constants.py </p>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>langroid/utils/logging.py </p>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_logger","title":"<code>setup_logger(name, level=logging.INFO)</code>","text":"<p>Set up a logger of module <code>name</code> at a desired level.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>module name</p> required <code>level</code> <code>int</code> <p>desired logging level</p> <code>logging.INFO</code> <p>Returns:</p> Type Description <code>logging.Logger</code> <p>logger</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger:\n\"\"\"\n    Set up a logger of module `name` at a desired level.\n    Args:\n        name: module name\n        level: desired logging level\n    Returns:\n        logger\n    \"\"\"\nlogger = logging.getLogger(name)\nlogger.setLevel(level)\nif not logger.hasHandlers():\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nreturn logger\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_loggers_for_package","title":"<code>setup_loggers_for_package(package_name, level)</code>","text":"<p>Set up loggers for all modules in a package. This ensures that log-levels of modules outside the package are not affected.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>main package name</p> required <code>level</code> <code>int</code> <p>desired logging level</p> required Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_loggers_for_package(package_name: str, level: int) -&gt; None:\n\"\"\"\n    Set up loggers for all modules in a package.\n    This ensures that log-levels of modules outside the package are not affected.\n    Args:\n        package_name: main package name\n        level: desired logging level\n    Returns:\n    \"\"\"\nimport importlib\nimport pkgutil\npackage = importlib.import_module(package_name)\nfor _, module_name, _ in pkgutil.walk_packages(\npackage.__path__, package.__name__ + \".\"\n):\nmodule = importlib.import_module(module_name)\nsetup_logger(module.__name__, level)\n</code></pre>"},{"location":"reference/utils/system/","title":"system","text":"<p>langroid/utils/system.py </p>"},{"location":"reference/utils/system/#langroid.utils.system.rmdir","title":"<code>rmdir(path)</code>","text":"<p>Remove a directory recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to directory to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a dir was removed, false otherwise. Raises error if failed to remove.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def rmdir(path: str) -&gt; bool:\n\"\"\"\n    Remove a directory recursively.\n    Args:\n        path (str): path to directory to remove\n    Returns:\n        True if a dir was removed, false otherwise. Raises error if failed to remove.\n    \"\"\"\nif not any([path.startswith(p) for p in DELETION_ALLOWED_PATHS]):\nraise ValueError(\nf\"\"\"\n        Removing Dir '{path}' not allowed. \n        Must start with one of {DELETION_ALLOWED_PATHS}\n        This is a safety measure to prevent accidental deletion of files.\n        If you are sure you want to delete this directory, please add it \n        to the `DELETION_ALLOWED_PATHS` list in langroid/utils/system.py and \n        re-run the command.\n        \"\"\"\n)\ntry:\nshutil.rmtree(path)\nexcept FileNotFoundError:\nlogger.warning(f\"Directory '{path}' does not exist. No action taken.\")\nreturn False\nexcept Exception as e:\nlogger.error(f\"Error while removing directory '{path}': {e}\")\nreturn True\n</code></pre>"},{"location":"reference/utils/output/","title":"output","text":"<p>langroid/utils/output/init.py </p>"},{"location":"reference/utils/output/printing/","title":"printing","text":"<p>langroid/utils/output/printing.py </p>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.PrintColored","title":"<code>PrintColored(color)</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>def __init__(self, color: str):\nself.color = color\n</code></pre>"},{"location":"reference/vector_store/","title":"vector_store","text":"<p>langroid/vector_store/init.py </p>"},{"location":"reference/vector_store/base/","title":"base","text":"<p>langroid/vector_store/base.py </p>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore","title":"<code>VectorStore(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract base class for a vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def __init__(self, config: VectorStoreConfig):\nself.config = config\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n\"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.list_collections","title":"<code>list_collections()</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self) -&gt; List[str]:\n\"\"\"List all collections in the vector store.\"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace the collection if it already exists. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\nif collection_name not in self.list_collections() or replace:\nself.create_collection(collection_name, replace=replace)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace the collection if it already exists. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of document ids.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n\"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/chromadb/","title":"chromadb","text":"<p>langroid/vector_store/chromadb.py </p>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB","title":"<code>ChromaDB(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def __init__(self, config: ChromaDBConfig):\nsuper().__init__(config)\nself.config = config\nemb_model = EmbeddingModel.create(config.embedding)\nself.embedding_fn = emb_model.embedding_fn()\nself.client = chromadb.Client(\nchromadb.config.Settings(\n# chroma_db_impl=\"duckdb+parquet\",\npersist_directory=config.storage_path,\n)\n)\nif self.config.collection_name is not None:\nself.create_collection(\nself.config.collection_name,\nreplace=self.config.replace_collection,\n)\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.list_collections","title":"<code>list_collections()</code>","text":"<p>List non-empty collections in the vector store.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of non-empty collection names.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def list_collections(self) -&gt; List[str]:\n\"\"\"\n    List non-empty collections in the vector store.\n    Returns:\n        List[str]: List of non-empty collection names.\n    \"\"\"\ncolls = self.client.list_collections()\nreturn [coll.name for coll in colls if coll.count() &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection in the vector store, optionally replacing an existing     collection if <code>replace</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to create or replace.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace an existing collection. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Create a collection in the vector store, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create or replace.\n        replace (bool, optional): Whether to replace an existing collection.\n            Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\nself.collection = self.client.create_collection(\nname=self.config.collection_name,\nembedding_function=self.embedding_fn,\nget_or_create=not replace,\n)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/","title":"qdrantdb","text":"<p>langroid/vector_store/qdrantdb.py </p>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB","title":"<code>QdrantDB(config)</code>","text":"<p>             Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def __init__(self, config: QdrantDBConfig):\nsuper().__init__(config)\nself.config = config\nemb_model = EmbeddingModel.create(config.embedding)\nself.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\nself.embedding_dim = emb_model.embedding_dims\nself.host = config.host\nself.port = config.port\nload_dotenv()\nkey = os.getenv(\"QDRANT_API_KEY\")\nurl = os.getenv(\"QDRANT_API_URL\")\nif config.cloud and None in [key, url]:\nlogger.warning(\nf\"\"\"QDRANT_API_KEY, QDRANT_API_URL env variable must be set to use \n            QdrantDB in cloud mode. Please set these values \n            in your .env file. \n            Switching to local storage at {config.storage_path}             \"\"\"\n)\nconfig.cloud = False\nif config.cloud:\nself.client = QdrantClient(\nurl=url,\napi_key=key,\ntimeout=config.timeout,\n)\nelse:\nself.client = QdrantClient(\npath=config.storage_path,\n)\n# Note: Only create collection if a non-null collection name is provided.\n# This is useful to delay creation of vecdb until we have a suitable\n# collection name (e.g. we could get it from the url or folder path).\nif config.collection_name is not None:\nself.create_collection(\nconfig.collection_name, replace=config.replace_collection\n)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.list_collections","title":"<code>list_collections()</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self) -&gt; List[str]:\n\"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n    \"\"\"\ncolls = list(self.client.get_collections())[0][1]\ncounts = [\nself.client.get_collection(collection_name=coll.name).points_count\nfor coll in colls\n]\nreturn [coll.name for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to create.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace an existing collection with the same name. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\ncollections = self.list_collections()\nif collection_name in collections:\ncoll = self.client.get_collection(collection_name=collection_name)\nif coll.status == CollectionStatus.GREEN and coll.points_count &gt; 0:\nlogger.warning(f\"Non-empty Collection {collection_name} already exists\")\nif not replace:\nlogger.warning(\"Not replacing collection\")\nreturn\nelse:\nlogger.warning(\"Recreating fresh collection\")\nself.client.recreate_collection(\ncollection_name=collection_name,\nvectors_config=VectorParams(\nsize=self.embedding_dim,\ndistance=Distance.COSINE,\n),\n)\ncollection_info = self.client.get_collection(collection_name=collection_name)\nassert collection_info.status == CollectionStatus.GREEN\nassert collection_info.vectors_count == 0\nif settings.debug:\nlevel = logger.getEffectiveLevel()\nlogger.setLevel(logging.INFO)\nlogger.info(collection_info)\nlogger.setLevel(level)\n</code></pre>"}]}